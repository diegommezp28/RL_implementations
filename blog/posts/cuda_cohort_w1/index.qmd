---
title: "A Guide on Multithreading and Multiprocessing: from the OS Kernel to ML Applications in Python."
description: "A deep dive into multithreading and multiprocessing, its relation with the OS, and how to use it in Python for speeding up Machine Learning tasks."
author: "Diego Andrés Gómez Polo"
date: "2/15/2024"
# draft: false
format:
  html:
    code-fold: true
    code-summary: "Show source"
    code-copy: true
    smooth-scroll: true
    html-math-method: katex
    highlight-style: pygments
    # mainfont: Lora
    fontsize: 16pt
    grid:
      body-width: 1000px
    css: 
      - styles.css
bibliography: ref.bib
csl: ieee-with-url.csl
# title-block-banner: images/banner.png
# title-block-banner-color: black
categories: ["en", "ML", "python", "multithreading", "multiprocessing"]
# image: images/thumbnail.png
toc: true
toc-location: left-body
comments: 
  utterances:
    repo: diegommezp28/RL_implementations
    label: blog-comments
    theme: github-dark-orange
    issue-term: pathname
website:
  # open-graph: 
  #   image: images/thumbnail.png
  twitter-card: true
editor:
  render-on-save: true
filters:
  - parse-latex


---

:::{.callout-important}
**DRAFT - WORK IN PROGRESS**
:::

## Motivation

If you've worked in software for long enough, you've probably heard in the wild people throwing around terms like *multithreading*, *multiprocessing*, *asynchronous programming*, *concurrency*, *parallelism*, and so on. Often, and mistakenly, these terms are used interchangeably, to refere to stuff thet happens *'at the same time'* in a computer program. You will see in this blog how most of the time this *'parallel behaviour'* is actually a carefully orchestrated illusion by the OS kernel and the CPU that lies at the heart of your computer. You will also learn how these concepts apply to Python and ML tasks in both CPU-bound and I/O-bound scenarios (don't worry, we will define these terms later ;D) and how a proper understanding of these concepts can help you speed up your ML tasks.

::: column-margin
**Personal Motivation:**

The contents of this blog are mostly based on the first 2 weeks of the CUDA Cohort by [Cohere for AI], an OpenScience Community which i am part of.

Kudos to the C4AI team for the amazing content and the great community they are building.
:::

  [Cohere for AI]: https://cohere.com/research

::: callout-warning
## Warning

Since this blog is in part a note to myself and in part a guide for others, there will be mixed didacticle and highly technical content. I will try to keep a balance and annotate the most technical parts with a warning like this one, so you can skip them if you are not interested in the nitty-gritty details.
:::

## Parallelism vs Concurrency

Sometimes, many programmers, even experienced ones, think that by adding more threads to a program it will magically become faster, and is not rare that when they actually do this, the program becomes slower, and they are left scratching their heads, questioning every piece of knowledge they have recieved about how a computer should work.

Often times, the essence of this confussion is due to a misunderstanding of these two concepts: *Parallelism* and *Concurrency*.

There are many great resources on the internet which try to explain the difference between the two, but in my opinion regularly they actually get it wrong. A good explanation i've found is by [Rob Pike] in his talk [Concurrency is not Parallelism]. In this talk, Rob Pike explains that *concurrency* is about **dealing with** lots of things at once, while *parallelism* is about **doing** lots of things at once.

  [Rob Pike]: https://en.wikipedia.org/wiki/Rob_Pike
  [Concurrency is not Parallelism]: https://youtu.be/oV9rvDllKEg?si=djukEXBpXNq5hAyR

This may seem rather abstract, so to clear things out, i am aspiring to have a very precise wording in my explanations: Concurrency is a property of the *structure* of a program, while Parallelism talks about the *execution* of a program. A concurrent program may be run completely sequentially, and most of the time this is the case, but it has a structure such that parts of it ***can*** be executed in parallel, potentially making it faster.

This is best seen with the following diagram:

![Structure of a concurrent programm]

  [Structure of a concurrent programm]: images/Conc%20vs%20Parall.svg {#fig-conc-struct .column-body-outset width="800px"}

In @fig-conc-struct we can see the abstract representation of a program that has to complete many tasks ($T_0, T_0^1, T_0^2, \cdots, T_2, T_3$) before exiting.

Crucially, this is **NOT** representing how it runs, but it represents the *dependencies* between the tasks. For example, task $T_2^1$ needs that $T_1^1$ is completed before it runs but doesn't really care about $T_0^3$, whether it has already run, will run or will never run, in the same way, $T_0^3$ needs for $T_0$ to be completed before it runs but doesn't really care about $T_2^1$.

::: column-margin
**A Note for the Data Engineers**

Those of you who have worked with Directed Acyclic Graphs (DAGs) in the context of data processing frameworks like [Apache Airflow], [Dagster], or [Dask], may have already identified the structure of a concurrent program as a DAG. This is not a coincidence, as it is precisely the exact same thing, where in this case the nodes are the instructions of the program and the edges are the dependencies between them.

You will see in the following section that your good old Airflow Scheduler is very similar to the OS Kernel Scheduler, but on a higher level of abstraction.
:::

  [Apache Airflow]: https://airflow.apache.org/
  [Dagster]: https://dagster.io/
  [Dask]: https://www.dask.org/

The important observation here is that because of this lack of dependency between $T_0^3$ and $T_2^1$, they are ***suited for parallel execution*** and if executed as such, can potentially make the program faster.

For a more intuitive view, you can think of the tasks that a program runs like the functions it calls, some functions require a prior call to other functions to run properly (probably the function `read_file` needs that we first call a function `open_file`) and some functions don't really care about the result of others (you can probably run `print("Hello World")` at any point in your program without any problem). [^1]

[^1]: If we get really precise, the tasks in @fig-conc-struct are actually CPU-level instructions, but for the sake of simplicity we will think of them as functions. For now...

Understanding what parts of your program constitute a block of tasks ***suited for parallel execution*** is the first step to make it faster.

I hope is also clear by now that *Parallelism* is just a synonym for *Parallel Execution*. So, indeed, this is the proper word to use when you want to refer to something *"running at the same time"* in a computer program.

Now, let's step away from the world of theory and actually get to know our computers...

If our CPUs had unlimited parallelism, we could just throw all the tasks *suited for parallel execution* at it and it would run them all at the same time, making our programs run faster. But, as you will see in the next section, this is not the case, and we need to understand how our computers work to make the most out of them.

I am crossing my fingers so that, when you finish reading the next section, you will not be that poor programmer scratching his head, after creating 128 threads in a Core i5 processor.

> **TL;DR:** Concurrency is about the structure of a program, while Parallelism is about its execution

::: {.callout-warning .column-body-outset collapse="false"}
## Concurrency, DAGs and Strict Partial Orders

If you have a little bit more of a formal background you may have already identified that the concept of *Concurrent Structure* later presented can be precisely defined in terms of basic set theory and binary relations. In concrete, the structure of a concurrent program can be seen as a [Strict Partial Order] where the elements of the set are the tasks $t \in T$ of the program and the relation $R \subset T \times T$ is the *happens-before* relation, where $\{t, t'\} \in R$ **iff** we have that $t$ *happens-before* of $t'$. Moreover, 2 tasks $t$ and $t'$ are *Suited for parallel execution if and only if they are not comparable in the partial order* , that is, $\{t, t'\} \notin R$ and $\{t', t\} \notin R$.

It is also worth noting that the notation used in @fig-conc-struct ist **not** generalizable to all concurrent programs. If it were, that would imply the existence of a constant time algorithm $\mathcal{O}(1)$ with linear space $\mathcal{O}(E)$ which finds if two nodes are connected in a [Directed Acyclic Graph] (DAG) *without* executing any path traversal algorithm. This would be the greatest solution ever discovered to calculating the [Transitive Clousure] of a Graph.... (Indeed this is me justyfing my simplistic notation :p)
:::

  [Strict Partial Order]: https://en.wikipedia.org/wiki/Partially_ordered_set#Strict_partial_orders
  [Directed Acyclic Graph]: https://en.wikipedia.org/wiki/Directed_acyclic_graph#:~:text=A%20directed%20acyclic%20graph%20is,a%20path%20with%20zero%20edges
  [Transitive Clousure]: https://en.wikipedia.org/wiki/Transitive_closure#In_graph_theory

## Parrallelism and Concurrency in your Computer

Identifying which parts of your program are suited for parallel execution is half of the recipe. let's see the other half.

The CPU or Central Processing Unit is the core component of your machine that is doing the heavy lifting when you run a program. It is the one[^2] doing all the math and logical operations necessary for providing you with a fully rendered and interactive screen running the latest versions of Chrome, Safari, Firefox, or, god forbid, Internet Explorer. But how does it actually do this?

[^2]: Let's forget about GPUs for now, we will cover that in other blogs. Also, in my defense, we lived perfectly okay without them for half of the digital era. ;p

Belive me when i tell you, this is close to magic, and for non-technical folks, it is actually magic, i have seen people on the internet saying that humans could have never invented such a complicated device, and that is the proof that aliens exist and have visited us. But, as you will see, it is actually a very well thought out and engineered piece of technology.

> Yeah, probably most of the magic is on the [Photolithography] process, and the aliens are the [dutch], but i am not that kind of engineer, so i will stick to the software side of things.

  [Photolithography]: https://en.wikipedia.org/wiki/Photolithography
  [dutch]: https://en.wikipedia.org/wiki/ASML_Holding

### CPU Basics

Your CPU consists of three main components: the **Control Unit**, the **Arithmetic Logic Unit (ALU)**, and the **Registers**. This design follows the [von Neumann architecture], which is the basis for most modern computers.

  [von Neumann architecture]: https://en.wikipedia.org/wiki/Von_Neumann_architecture

<!--  Diagram-->

![Simple Diagram of CPU with Von Neumann Architecture]

  [Simple Diagram of CPU with Von Neumann Architecture]: images/CPU_simple.svg {#fig-cpu-simple width="70%"}

Like the conductor of an orchestra swinging the baton, the **Control Unit** is the part of the CPU that reads and interprets instructions from memory. It then directs the operation of the other units by providing control signals. The **Arithmetic Logic Unit (ALU)** is the part of the CPU that performs arithmetic and logical operations. The **Registers** are small, fast storage locations within the CPU that hold data temporarily during processing.

Unlike an orchestra, the tempo or speed in a CPU is not set by the conductor or the *Control Unit*, but by a specific hardware component called the [Clock]. The clock is a device that generates a signal at a constant frequency that is used to synchronize the operation of the CPU. The frequency of the clock is measured in Hertz (Hz) and is the number of clock cycles per second

  [Clock]: https://en.wikipedia.org/wiki/Clock_generator

Your CPU also interacts with external storage and I/O devices via the **Buses** which are basically wires connecting things. There are many kinds of buses which allow the CPU to talk to different components like the RAM, the SSD, the Keyboard, the Display, and so on.

> *But, why do we care about the Registers, Clock and so on if we are talking about parallelism and concurrency?* Bare with me, we are getting there.

Well, it turns out that even though this design is capable of supporting a complete system running many apps, it is ***NOT*** running them all at the same time. In fact, in the simplest case, a given CPU core is only capable of running a single instruction of a very limited [set of instructions] at a time.

  [set of instructions]: https://en.wikipedia.org/wiki/Instruction_set_architecture

Everything your computer ever does must be translated, in one way or another, to a series of these simple instructions [^3], which are interpreted by the Control Unit and then executed either by the Control Unit itself when they involve control flow operations or by the ALU when they involve arithmetic or logical operations.

[^3]: That is why we have [compilers] and why a program needs to be compiled for every different target [Instruction Set Architecture], like [x_64], [x_86], [RISC], etc, since those have different instruction sets

  [compilers]: https://en.wikipedia.org/wiki/Compiler
  [Instruction Set Architecture]: https://en.wikipedia.org/wiki/Comparison_of_instruction_set_architectures
  [x_64]: https://en.wikipedia.org/wiki/X86-64
  [x_86]: https://en.wikipedia.org/wiki/X86
  [RISC]: https://en.wikipedia.org/wiki/Reduced_instruction_set_computer

But clearly you have seen your computer running many things at the same time, right? The music in the background doesn't just stop playing if you open a new tab in your browser, right? Well, in some sense, it does, but it is so fast that you don't notice it.

![Interleaving of Program Instructions]

  [Interleaving of Program Instructions]: images/CPU_interleave.png {#fig-interleaving width="85%"}

To give you the illusion that your computer is running many things at the same time, the OS kernel and the CPU have to do some magic...

In the dark old ages, *prior to 2005*, there was no parallelism AT ALL in your personal PC, scary times indeed my friends :c, all the consumer CPUs in the market followed the standard single core architucture we just described. For them to run many applications without one completly freezing waiting for the other to finish, we basically had a single option, to interleave between programs, taking turns executing some of their instructions: a little bit of word here, OS kernel, some more excel there, OS kernel, let's do networking stuff, OS kernel, and so on, really, really, **REALLY FAST**, and just like that, creating a perfectlly orchestrated illusion of parallelism while providing you with a fully interactive experience.

And if you wanted faster CPUs you just needed to increase the frequency of the clock, interleaving ever faster, without worrying about true parallelism and silly things like that.

#### Multi-core CPUs

But it turns out that the universe has some physical limitations, and we hit a wall of how many clocks we could squeeze in a second back in the early 2000s. This is known as the [Breakdown of Dennard's scaling] and it is the reason why we don't have 10 GHz CPUs in our laptops, basically because they would melt.

  [Breakdown of Dennard's scaling]: https://en.wikipedia.org/wiki/Dennard_scaling#Breakdown_of_Dennard_scaling_around_2006

So instead of increasing the frequency of the clock, we started adding more cores to the CPU, striving for **True Parallelism**.

Thanks to this new design, then we didn't have a single core running many things, we then had many cores running many things, and the OS kernel has to orchestrate the execution of all of them, giving them time to run their instructions and taking them out of the CPU at specific interlvals, to let the rest of programs execute some of their instructions. What we saw in @fig-interleaving now looks more like this:

![Interleaving Instructions in a Multi-core CPU]

  [Interleaving Instructions in a Multi-core CPU]: images/Multi_core_cpu.png {#fig-interleaving-multi-core width="85%"}

Stuff stills gets interleaved, but now we have many interleaving streams, allowing for true parallelism.

Crearly the OS kernel has to be a little bit more sophisticated now, implementing what people call [Symmetric Multi Processing], but the basic idea is the same, it has to give execution time to every program that wants to run, and it has to do it in a way that is fair and efficient.

  [Symmetric Multi Processing]: https://linux-kernel-labs.github.io/refs/heads/master/lectures/smp.html

*'Fair and efficient'* is a very broad term, and it is actually a very hard problem to solve, but the OS kernel has some tools to help it with this task, and one of the most important is the ***Scheduler***, which assigns execution time to the ***Threads*** of a ***Process***. We will talk about all of this terms in the following sections, the problems that arise with true parallelism, and how to solve them.

#### Hardware Multi-threading

Before getting into Processes, Threads, Context Switches and all, i shall admit that i've been cutting corners while doing @fig-interleaving and @fig-interleaving-multi-core. It turns out that in modern architectures the ALU is not the single [Execution Unit], but we have a collection of highly specialized circuits that can perform different operations, for example, we can have a circuit for integer operations while another one for floating point operations, called the [Floating-point Unit]. The specific units of execution depends on the [Microarchitecture] of the CPU, but the important thing is that we have more than one.

  [Execution Unit]: https://en.wikipedia.org/wiki/Execution_unit
  [Floating-point Unit]: https://en.wikipedia.org/wiki/Floating-point_unit
  [Microarchitecture]: https://en.wikipedia.org/wiki/Microarchitecture

What this implies in practice is that if we only run one instruction per clock cycle, most likely some part of the processor will be idle, and we would not be making the most out of our hardware.

That is why some great geniuses came up with [Super Scalar Processors] and [Simultaneous Multi-threading] (SMT) which allows for the execution of multiple instructions per clock cycle, by having the CPU dispatch multiple instructions of the [Instruction Queue] per clock cycle, as long as those instructions are meant to be executed by different circuits. [-@Bovet_Cesati_2001]

  [Super Scalar Processors]: https://en.wikipedia.org/wiki/Superscalar_processor
  [Simultaneous Multi-threading]: https://en.wikipedia.org/wiki/Simultaneous_multithreading
  [Instruction Queue]: https://en.wikipedia.org/wiki/Prefetch_input_queue

![CPU Core with Two Execution Threads]

  [CPU Core with Two Execution Threads]: images/CPU_core_2_threads.svg {#fig-hyperthreading width="85%"}

With this architecture, each CPU core can improve its parallelism, without making too many changes to the overall design, and without increasing the power consumption too much.

On most of today's consumer grade CPUs, the CPU exposes 2 execution contexts per core, see @fig-hyperthreading, which are commonly refer to as ***Threads***, and the OS kernel usually treats them as such, giving them execution time as if they were different cores. [-@hyperthreading-intel]

::: callout-note
## Further Reading

We are not Electronic Engineers in here, nor chip designers (or maybe you are, who knows), but this is a fascinating topic, so if you want a deeper dive into this subject, i recommend you resources like [WikiChip] and the [Intel Developer Manual] for a more detailed explanation of the microarchitecture of modern CPUs.
:::

  [WikiChip]: https://en.wikichip.org/wiki/intel/microarchitectures/skylake_(client)#Individual_Core
  [Intel Developer Manual]: https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html

### Programs, Processes, and Threads

From now on, we will, *mostly*, talk about software. From a low level with the OS Kernel, to a very high level with Python, but still, software.

In the last chapter we saw how in most modern CPUs we do have a way of doing actual parallelism, by using multiple CPU cores. Moreover, even inside a single CPU core, we can have multiple *Threads* of execution, which can be used to improve parallelism. But, we also saw that the *OS Kernel* has to orchestrate the execution of all of them.

This magic *OS Kernel* is just another piece of software, but it is a very special one, with all the privileges to do whatever it wants with the hardware, and it is the one that gives execution time to every program that wants to run.

So, for the OS kernel to nicely orchastrate all of this madness, it needs some structure, some way of managing the state of your application and the state of the hardware.

To do so, it creates some very important abstractions for each *Program* you wish to run, and even for itself, and these are the ***Processes*** and ***Threads***.

> Rembember we are talking about software in here, so the term *Thread* is not the same as the physical things we just talked about, but it is very closly related.

Every piece of information about your program, its state, the things it needs to properly run, to be safely interrupted by the Kernel and to resume execution, are nicely packed in these abstractions, which formally are *Data Structures* in the Kernel's memory, but for us, they are just *Processes* and *Threads*.

<!-- There must be some form of predefined structure that gives all the control to the OS kernel and is able to give execution time to every program that wants to run -->

#### Definitions

Let's be precise about these concepts, since they are the key parts a software engineer actually needs to understand to properly program concurrent applications.

A ***Program*** is a set of instructions that are meant to be executed by the CPU. This is the code you write in your favorite programming language, which is then compiled or, in the case of Languages like Python, interpreted by another program called the *Interpreter*. Anyways, the important thing is that a Program is just machine code that the CPU can execute (think, for example, of all of those `.exe` files).

![Process Control Block and Thread Control Block]

  [Process Control Block and Thread Control Block]: images/pcb_tcb.svg {#fig-pcb-tcb width="85%"}

A ***Process*** is an instance of a Program that is being executed by the CPU. This is the actual running of the Program, with all the data structures, memory, and resources that it needs to run properly. The OS Kernel creates a Process for every instance of a Program that you run (if you click on a `.exe` file many times, your OS will create a separate process for each time).

If you open the *Task Manager* in Windows, the *Activity Monitor* in MacOS or run `systemctl` in Linux, you will see a list of all the Processes that are currently running in your computer, and you will see that there are many of them, even if you are not running many applications. This is because the OS Kernel also creates Processes for itself, for the *System Services*, for the *Drivers*, and for every running instance of your favorite browser, text editor, or game.

A ***Thread*** means an *execution thread* within a Process, i.e, a series or list of instructions, from the ones that the Process has in memory, that is meant to be executed.

Crearly each Thread needs the specifics of which instructions from the whole set to execute, and some resources to do it, that is why, for each Thread we also have indepedent [*Program Counter*], [*Registers*], and [*Stack*]. But Threads of the same Process share the same *Memory Space*, including, as said before, the actual code, also, the *File Descriptors* and the *Heap*.

  [*Program Counter*]: https://en.wikipedia.org/wiki/Program_counter
  [*Registers*]: https://en.wikipedia.org/wiki/Processor_register
  [*Stack*]: https://en.wikipedia.org/wiki/Call_stack

*Threads* are also the smallest unit of execution, meaning the OS Kernel will Schedule Threads, not Processes, to run in the CPU. This is because Threads are much lighter than Processes, since they share the same Memory Space, and the OS Kernel can easily switch between them, without having to do a lot of work to save and restore the state of the Process.

::: {.callout-warning collapse="false"}
# Process Control Block (PCB): The Actual Data Structure of a Process

PCB is the generic term used to refer to the Data Structure that an OS employs in order to manage Processes. In the case of the Linux Kernel (the only major one where we can see the code), it is called the [*task_struct*] and it is a very complex structure that holds all the information about a Process. @pcb_5
:::

  [*task_struct*]: https://elixir.bootlin.com/linux/v6.2-rc1/source/include/linux/sched.h#L737

::: {.callout-warning collapse="false"}
# Thread Control Block (TCB): The Actual Data Structure of a Thread

Likewise, TCB is the generic term used to refer to the Data Structure that an OS employs in order to manage Threads. Linux implements it by kind of cheating, because for the Linux Kernel, a Thread is just another *task_struct*, the same as before, but with a pointer to a parent *task_struct* that is the [*parent Process*] for the Thread, also, all the [*children Threads*] of a Process will share some resources but will have some differences specified in the [*thread_info*] struct, and you can create a new children Thread by calling the [`clone()`] system call.

In this sense you may see that in Linux, we have a tree-like structure of tasks, with the root being the first process (PID 0), i.e, the [*init*] process, and every other process being an ancestor of the *init* process, finally, the Threads will then be the leaves of this [tree] of tasks. @pcb_1 @pcb_2 @pcb_3 @pcb_4 @pcb_5

This is in Linux, in other OSs, like Windows, Threads are a separate concept from Processes, and they have their own Data Structure, but the idea is the same, they hold all the information about the Thread, like the Registers, the Stack, the Program Counter, and so on, as specified in @fig-pcb-tcb, but between [*sibling Threads*], they will share the same Memory Space, the same File Descriptors, and the same Heap.
:::

  [*parent Process*]: https://elixir.bootlin.com/linux/v6.2-rc1/source/include/linux/sched.h#L975
  [*children Threads*]: https://elixir.bootlin.com/linux/v6.2-rc1/source/include/linux/sched.h#L983
  [*thread_info*]: https://elixir.bootlin.com/linux/v6.2-rc1/source/arch/alpha/include/asm/thread_info.h#L15
  [`clone()`]: https://man7.org/linux/man-pages/man2/clone.2.html
  [*init*]: https://en.wikipedia.org/wiki/Init#:~:text=Init%20is%20a%20daemon%20process,automatically%20adopts%20all%20orphaned%20processes
  [tree]: https://linuxhandbook.com/show-process-tree/
  [*sibling Threads*]: https://elixir.bootlin.com/linux/v6.2-rc1/source/include/linux/sched.h#L984

::: {.callout-warning collapse="false"}
# Protection Rings and General Hierarchy

The core distinction between the kernel process and all other processes is implemented at the hardware level by using [*Protection Rings*]. These are a set of hardware-enforced protection domains that are used to control the CPU's access to memory, I/O devices, and other resources. The OS kernel runs in the most privileged ring, usually ring 0, while user processes run in a less privileged ring, usually ring 3. This is a security feature that prevents user processes from accessing system resources directly and ensures that the OS kernel has full control over the system.

![Rings of Protection. Fig Source: [Wikipedia]][1]

Modern microarchitectures have many more than 2 protection rings, but mostly for retrocompatibliity reasons, most Operating Systems including Windows, MacOS and Linux, only use 2, the most privileged and the least privileged, where ring 0 is the *Supervisor Mode*, and ring 3 is the *User Mode*. @protection_rings_1 @protection_rings_2

![Only 2 Protection Rings are used in modern Oss. Fig Source: [Wikipedia]](https://upload.wikimedia.org/wikipedia/commons/c/c7/Most_common_protection_rings.svg){#fig-2-protection-rings width="75%"}
:::

  [*Protection Rings*]: https://en.wikipedia.org/wiki/Protection_ring
  [Wikipedia]: https://upload.wikimedia.org/wikipedia/commons/2/2f/Priv_rings.svg
  [1]: https://upload.wikimedia.org/wikipedia/commons/2/2f/Priv_rings.svg {#fig-protection-rings width="75%"}

### OS Kernel Scheduler

We are clear by now that the OS Kernel is the sole administrator of resources in our computers. There are many types of these resources, from memory and files, to the CPU itself. The latter is our subject matter for this chapter.

Access to CPU is quite valuable, most Threads usually want it, so the OS Kernel *Scheduler* has to be very careful in how it gives it.

Processes in modern OSs fall in one of, usually, 5 states, commomly known as the *5 State Model*. These states are: Init, Ready, Running, Waiting and Finished. @process_states_1

![5 State Model. From @process_states_1](images/5_state_model.svg){#fig-5-state-model width="85%"}

:::{.callout-note}
The *5 State Model* is an asbtraction and simplification of the actual states. For example, the Linux Kernel implements de following states: `TASK_RUNNING`, `TASK_INTERRUPTIBLE`, `TASK_UNINTERRUPTIBLE`, `TASK_STOPPED`, `TASK_ZOMBIE`. @process_states_2

:::

The Scheduler will look for the ones that are ready to run, and will give them execution time in the CPU. The way it gives access to CPU is by doing a *Context Switch*, which is the process of saving the state of the currently running Thread, and loading the state of the Thread that is going to run. This is a very expensive operation, since it involves saving and restoring the state of the Registers, the Program Counter, the Stack, and so on, but it is necessary to give execution time to every Thread that wants to run.

#### Time Slices

To have a somewhat simple way of calculating the time a Thread will be allowed to run, the Scheduler has a quanta or minimum, discrete, amount of time that it can give, called the *Time Unit*. So, the assignment of execution time for a given Thread will always be calculated as a fixed integer multiple of this Time Unit.

When a Thread is given Time Units of execution, it is called a *Time Slice*, and the Scheduler will preemptively take the Thread out of the CPU when it has used up all of its *Time Slice*, and will give the CPU to another Thread that is ready to run. This is the essence of a *Preemptive Scheduler*, which is the most common type of Scheduler in modern OSs.

> Note that a time slice **Is NOT** a promise of uninterrupted execution time, but a promise of a minimum amount of total available execution time. The Scheduler can take the Thread out of the CPU at any moment, even before the Time Slice is up, if it decides that another Thread needs to run.

#### Scheduling Algorithm

The way the Scheduler decides which Thread to run next is called the [*Scheduling Algorithm*]. There are many of these, and they are usually classified in two categories: ***Preemptive*** and ***Non-Preemptive***. @priority_levels_1, @scheduler_1, @scheduler_2

  [*Scheduling Algorithm*]: https://www.geeksforgeeks.org/cpu-scheduling-in-operating-systems/

A [Preemptive Scheduler] is pessimistic, and it assumes that any given Thread will run forever if left to its own devices, so they have to be taken forcefully out of the CPU to allow for efficient interleaving of Threads. On the other hand, a Non-Preemptive Scheduler is counting on the goodwill of the Thread to give up its resources from time to time to allow for interleaving, which is not very efficient, but it is much simpler to implement.

  [Preemptive Scheduler]: https://en.wikipedia.org/wiki/Preemption_(computing)

Modern Schedulers are preemptive, and they usually use **Priority Queues** and **Round Robin Scheduling** to decide which Thread to run next.

#### Priority Queues and Priority Levels

The Priority level is how the Scheduler assigns importance to all the Threads that are ready to run. The Thread with the highest priority level will be the one schedule to run next, and the Scheduler will modify the priority level of the Threads as they run, to give execution time to all of them and to ensure that no Thread is left behind.

![Simplified Priority Levels. Source @priority_levels_1](images/priority_levels.svg){#fig-priority-levels width="85%"}

The very specifics of the modifications and updates of priority levels vary by OS kernel implementations. For example, the linux kernel scheduler has been implemented with the [CFS or Completely Fair Scheduler] for the last 15 years, it utilizes a [Red-Black Tree] to store the Threads, and it uses the [Virtual Runtime] of the Threads to calculate the priority level.

  [CFS or Completely Fair Scheduler]: https://en.wikipedia.org/wiki/Completely_Fair_Scheduler
  [Red-Black Tree]: https://en.wikipedia.org/wiki/Red%E2%80%93black_tree
  [Virtual Runtime]: https://www.kernel.org/doc/html/latest/scheduler/sched-design-CFS.html

::: {.callout-warning collapse="false"}
### Earliest eligible virtual deadline first scheduling

[EEVDF] is the scheduler algorithm set to replace CFS. It was described almost 30 years ago, but was only merged in 2023 to the development version 6.6 of the kernel. @scheduler_3 @scheduler_4
:::

  [EEVDF]: https://en.wikipedia.org/wiki/Earliest_eligible_virtual_deadline_first_scheduling

#### Round Robin Scheduling

When the top priority level has been given to more than one Thread, like in @fig-priority-levels, the Scheduler will use a [Round Robin Scheduling] to decide which one to run next. This is a simple algorithm that gives a fixed amount of Time Units to each Thread in a circular fashion, and then starts again. Is desgined to resolve disputes between Threads of the same priority level, and to ensure that all Threads get execution time.

  [Round Robin Scheduling]: https://en.wikipedia.org/wiki/Round-robin_scheduling

![Simplified Round Robing Scheduler. Source @priority_levels_1](images/round_robin.svg){#fig-round-robin width="75%"}

::: {.callout-warning collapse="false"}
### Simulating a Round Robin Scheduler

Round Robin is a very simple, yet effective algorithm, and we can simulate its behaviour even by hand with a toy example:


:::{.LATEX}
**Exercise:**
Consider a system with 3 Threads A, B and C. The threads arrive in the order A, B, C and the arrival time are at 0ms for each thread. The time quantum is 2ms. The threads are scheduled using the Round Robin scheduling algorithm. The context switch time is negligible. The CPU burst times for threads A, B and C are 4ms, 8ms and 6ms respectively. Compute the turnaround time for each thread. 
:::

**Solution:** 
The threads are scheduled as follows:

![Example RR](images/tabla_round_robin.svg){#fig-rr-example width="100%"}

> This example exercise is a modification of a very similar one shown to me in the *Cuda Mini Cohort* at Cohere For AI.

- Thread A runs from 0ms to 2ms
- Thread B runs from 2ms to 4ms
- Thread C runs from 4ms to 6ms
- Thread A runs from 6ms to 8ms
- Thread B runs from 8ms to 10ms
- Thread C runs from 10ms to 12ms
- Thread B runs from 12ms to 14ms
- Thread C runs from 14ms to 16ms
- Thread B runs from 16ms to 18ms
  
The turnaround time for each thread is the time it takes to complete the execution of the thread from the time it arrives. The turnaround time for threads A, B and C are 8ms, 18ms and 16ms respectively. And the average turnaround time is 14ms.

:::

<!-- ### Primitives to Handle Concurrency

#### Mutexes or Locks

#### Semaphores

#### Condition Variables

#### Barriers -->


### Problems that Appear in Concurrent Programs

Concurrency is a powerfull tool to harness every bit of your CPU processing power but it comes with a lot of potential problems and hard to debug situations for the usual synchrounous minded programmer. Hopefully, the kinds of problems that appear are well documented and there are techniques to solve them. We will cover the most common ones in the next section.


#### Race Conditions

A Race Condition occurs when two or more Threads are trying to access and modify the same resource at the same time. Like we saw early, the scheduler may take a Thread out of the CPU at any moment, so 2 Threads wanting to modify the same resource may end up corrupting it, or worse, corrupting the whole program, because they are not aware of each other and not guaranteed to run in a specific order.

If i can say so myself, i think that Race Conditions are the root of all evil in concurrent programming. These are the main reason we have Synchronization Primitives (Like Mutexes, Semaphores, Barriers, etc), and the reason we have to be very careful when writing concurrent programs. 

When your Threads or Processes don't share data or resources, you usually won't have to worry about any of the problems that we will see next, but when they do, you do have to be very careful.


We can get a little bit ahead of ourselves and see a simple example of a forced race condition in Python:

```{python}
# | echo: true
# | code-fold: show
from threading import Thread
import random
import time


class Balance:
    def __init__(self, balance=200):
        self.balance = balance

    def deposit(self, amount):
        balance = self.balance
        time.sleep(random.random() / 10)  # <1>
        self.balance = balance + amount

    def withdraw(self, amount):
        balance = self.balance
        time.sleep(random.random() / 10)
        self.balance = balance - amount

    def get_balance(self):
        time.sleep(random.random() / 10)
        return self.balance


def race_condition():
    balance = Balance()
    t1 = Thread(target=balance.deposit, args=(100,))
    t2 = Thread(target=balance.withdraw, args=(50,))
    t1.start(), t2.start()
    t1.join(), t2.join()
    return balance.get_balance()


if __name__ == "__main__":
    balances = [race_condition() for _ in range(10)]
    print(balances)
```
1. Simulate possible context switch here

What is happening in here will conceptually be clearer by seeing @fig-withdrawal_no_lock, basically we have 2 Threads which are trying to modify the same resource, the `balance` attribute of the `Balance` class, and they are doing so without any kind of synchronization and whitout knowing about each other. Here we are forcing a context switch by calling `time.sleep(random.random() / 10)` in the middle of the method, but in a real world scenario, the context switch can happen at any moment, making it really hard to debug and reproduce the problem, since it may happen, literally, at one every 1000 or 10000 runs.

::: {#fig-withdrawal layout-ncol=2}

![Unsinchronized Access to Resource](images/withdrawal_no_lock.svg){#fig-withdrawal_no_lock}

![Sinchronized Access to Resource](images/withdrawal_lock.svg){#fig-withdrawal_lock}

The same race condition as in the code, with and without a synchronization primitive.
:::

Not only that, but even harder to debug is the fact that some of Python's syntatic sugar operations may hide the fact that we are dealing with multiple calls to the same resource, like in the case of the `+=` operator, which is actually a shorthand for `balance = balance + amount`, and it is not atomic, meaning that it is actually 3 operations: read the value of `balance`, sum `amount` to it, and write the result back to `balance`. Implying that the context switch can happen between any of these operations, and the result will be a corrupted `balance` attribute.

To solve this type of problems people invented synchronization primitives, like Mutexes, which basically are a way of telling the OS Kernel that a Thread is using a resource, and that no other Thread can use it until the first Thread is done with it.

We can change our code to use one of these primitives, like so:

```{python}
# | echo: true
# | code-fold: show
from threading import Thread, Lock
import random
import time


class Balance:
    def __init__(self, balance=200):
        self.balance = balance
        self.lock = Lock()

    def deposit(self, amount):
        with self.lock: # <1>
            balance = self.balance
            time.sleep(random.random() / 10)
            self.balance = balance + amount

    def withdraw(self, amount):
        with self.lock:
            balance = self.balance
            time.sleep(random.random() / 10)
            self.balance = balance - amount

    def get_balance(self):
        with self.lock: 
            time.sleep(random.random() / 10)
            return self.balance


def race_condition():
    balance = Balance()
    t1 = Thread(target=balance.deposit, args=(100,))
    t2 = Thread(target=balance.withdraw, args=(50,))
    t1.start(), t2.start()
    t1.join(), t2.join()
    return balance.get_balance()


if __name__ == "__main__":
    balances = [race_condition() for _ in range(10)]
    print(balances)


```
1. Use a Lock, the Python implementation of a Mutex, to synchronize access to the resource

Now, the `Lock` object is used to synchronize access to the `balance` attribute, and the `with` statement is used to acquire and release the Lock, making sure that only one Thread can access the resource at a time. This way, we can be sure that the `balance` attribute will not be corrupted, and that the result of the `race_condition` function will always be the same. See @fig-withdrawal_lock



#### Deadlocks

Using Locks solves Race Conditions but creates its own problems. I mean, if you think about it, many entities locking many stuff all at once should lead to problems, right? Well, indeed it does, namely, Deadlocks.

Deadlock are situations in which 2 or more entities, in this case Threads, are waiting for each other to release a resource, but none of them will release the resource until the other one does. Is like when you want to give the right of way to the other driver, but the other driver is also trying to give you the right of way, and you both end up stuck in the middle of the intersection.


::: {#fig-deadlocks layout-ncol=2}

![A Deadlock in a street intersection](images/deadlock_cars.svg){#fig-deadlock-cars}

![A Deadlock with 2 Threads](images/deadlock_threads.svg){#fig-deadlock-threads}

Illustrative representation of a Deadlock
:::

In computers this happen only if some specific properties apply to the Locks: The first one is that the Locks acquired are not preemtible, meaning that other Threads, including the Kernel, can not just take the resource away from the Thread that is holding it; the Second one is that the Locks are not acquired in a strict order, so the case may happen in which Threads want to aquire the same set of Locks but in a different order, and the Third one is that the Threads are waiting for each other to release the resource.

Given that these 3 conditions are met, the Threads will be stuck in a Deadlock, and the only way to solve it is to restart the program, or to kill the Threads that are stuck. 
If we don't want to basically kill the programs, the only ways to solve Deadlock is by attacking one of such necessary conditions. We can attack the first one by making the Locks preemtible, such that the OS Kernel can take the Lock away from a Thread that is holding it, and give it to another Thread that is waiting for it. Another way of tackling this first condition is to use a *Timeout* when acquiring a Lock, with the intention that if the Lock is not acquired in a specific amount of time, the Thread will give up and try again later.

For tackling the second necessary condition, we can force the Locks to be acquired in a strict order, defining a hierarchy of Locks, and making sure that every Thread acquires the Locks in the same order.

All of these solutions can work, but they are not easy to implement, and they are not always possible, so the best way to avoid Deadlocks is to be very careful when using Locks, and to always think about the consequences of acquiring a Lock.

:::{.callout-warning collapse="false"}
### Implementing a Lock Hierarchy in Python

In Python, the `Lock` object does not have a built-in way of defining a hierarchy of Locks, but we can implement it ourselves. Here is an example of how to do it:

```{python}
# | echo: true
# | code-fold: show
import threading
from time import sleep
from random import random

# Define locks
lock_A = threading.Lock()
lock_B = threading.Lock()
lock_C = threading.Lock()

# Create a lock hierarchy (order)
lock_order = {lock_A, lock_B, lock_C}


# Function to acquire locks in order
def acquire_locks():
    sleep(random() / 10)  # Simulate context switch
    for lock in lock_order:
        lock.acquire()


# Function to release locks in reverse order
def release_locks():
    sleep(random() / 10)  # Simulate context switch
    for lock in lock_order:
        lock.release()


# Example function using the locks
def thread_task(i):
    # Acquire locks in the defined order
    acquire_locks()

    try:
        # Critical section
        print(f"Thread {i} is executing with all locks acquired")
    finally:
        # Always release locks to avoid deadlocks
        release_locks()


# Create multiple threads
threads = []
for i in range(5):
    t = threading.Thread(target=thread_task, args=(i,))
    threads.append(t)
    t.start()

# Wait for all threads to complete
for t in threads:
    t.join()

```

:::


:::{.callout-warning collapse="false"}


#### Priority Inversion

Finally, we will talk about another problem that is of concern no so much to application developers but to OS Kernel developers; Priority Inversion.

This is a somewhat weird situation, first described back in the 80's in @priority_inversion_1 where you have a lower priority thread executing first and blocking a higher priority thread thanks to a resource that is being held and is impeding the higher priority thread to execute. 

Remember that the priority of threads are crucial for the scheduler to know which one should run first as seen in @fig-priority-levels, so if a lower priority thread is blocking a higher priority thread, the scheduler is not doing its job properly.



![Sort of Priority Inversion. Not that bad since are running as per designed.](images/priority_inversion.svg){#fig-priority-inversion width="50%"}

The situation we just described is not *that bad*, since even if the lower priority thread is executing, it is not doing enything out of the ordinary, i mean, it was programmed to take the lock of the shared resource, and it is doing so, yes it is blocking a higher priority thread, but it is not doing anything wrong, and when it finished executing, the higher priority thread will be able to run.

I want to enfatize, that when the application developer used the locks in both Thread A and C, he  knew ***in advanced*** that this very situation could happend, that is why is not *that bad*.

The really bad situation, the same that caused glitches in the mars rover back in 1997 @priority_inversion_mars_1, is when there is a medium priority thread involved, that will actually cause some unexpected behaviour of the system, that the application developer never thought of.



![Priority Inversion Worst Case. Definitely breaking expected behaviour.](images/priority_inversion_3_threads.svg){#fig-priority-inversion-3-threads width="60%"}

Look at @fig-priority-inversion-3-threads, first the lower priority Thread C aquires the resource the same way it did in @fig-priority-inversion, because of this, the higher priority Thread A is blocked waiting for the resource to be release, but in this case, a medium priority Thread B preempts Thread C, and starts running for as long as it wants, blocking C by priority and consequently, indirectly blocking the higher priority A. This is a very bad situation, because the system is not behaving as expected, and the higher priority Thread A is being blocked by something that was never designed to block it, leading to possible system failures.

Let's enfatize again that this is the ***really bad*** situation because the application developer never thought of Thread B blocking Thread A, so the code will not be prepared to handle this case, and the system may fail.

And this lack of preparation for the unexpected situation is what caused the Mars Rover to glitch, because in its case, when the higher priority Thread was blocked for a given amount of time, the scheduler detected a failure and went into safe mode, restarting the whole system, and causing a lot of problems. @priority_inversion_mars_2.


OS kernels have some ways of solving this problem, like the [*Priority Inheritance*](https://en.wikipedia.org/wiki/Priority_inheritance) in linux @priority_inversion_linux and the [*Priority Ceiling*](https://en.wikipedia.org/wiki/Priority_ceiling_protocol) as described in @priority_inversion_2. The former solves the problem by giving the priority of the higher priority thread, in our case Thread A, to the lower priority thread, Thread C, that is blocking it, and the latter solves the problem by assigning a priority to the shared resource, the Lock in our case, and making sure that the threads that are going to use it have a priority higher than the priority of the resource.  

Other approach that is used by Windows, is [*Random Boosting*](https://en.wikipedia.org/wiki/Random_boosting) @priority_inversion_windows, which is a way of giving a random boost to the priority of the lower priority thread, so it can finish faster and the higher priority thread can run.

> **TL;DR** Priority Inversion is an example of weird problems that can happen in concurrent programs, and it is a problem that is of concern to OS Kernel developers, not so much to application developers.

:::

<!-- cites:

pathfinder 1
pathfinder 2
lampson and redell
boost windows
pirority inheritance linux
 -->

### (Off-topic) How Does Your Computer Boot?

:::{.callout-warning}

This is a very off-topic section, but i think it is very interesting. If you don't think so, you can skip it.

:::

We have been talking about Threads and Processes, OS Kernels and Schedulers, under the asumption that they are already running in your PC, but how does your computer actually get to this state? How does the push of a button in your computer actually makes it run?

When you press the power button on your computer, a lot of things happen, and most of them are done by the [BIOS](https://es.wikipedia.org/wiki/BIOS) or [UEFI](https://en.wikipedia.org/wiki/UEFI) firmware, which is a very small program that is stored in a special memory chip in your motherboard, and that is executed by the CPU when you press the power button.

When you power on your computer, the CPU starts executing instructions from a predefined memory location known as [the reset vector](https://en.wikipedia.org/wiki/Reset_vector). This vector typically points to a small piece of firmware stored in a ROM or flash memory chip on the motherboard. In x86 systems, this firmware is known as the BIOS (Basic Input/Output System) or UEFI (Unified Extensible Firmware Interface).

 The BIOS or UEFI performs a series of hardware checks and initializations to ensure that essential components like RAM, storage devices, and peripherals are functioning correctly. It also identifies and initializes the CPU, sets up [interrupt handlers](https://en.wikipedia.org/wiki/Interrupt_handler), and performs other low-level hardware configurations.

 Once the hardware initialization is complete, the BIOS/UEFI locates and loads the [boot loader](https://en.wikipedia.org/wiki/Bootloader) into memory. The boot loader is a small program responsible for loading the operating system kernel into memory and transferring control to it. In Unix-based systems, the most commonly used boot loader is [GRUB (GRand Unified Bootloader)](https://en.wikipedia.org/wiki/GNU_GRUB).

 The boot loader typically resides in a specific location on the disk known as [the Master Boot Record (MBR)](https://en.wikipedia.org/wiki/Master_boot_record) or [the EFI System Partition (ESP)](https://en.wikipedia.org/wiki/EFI_system_partition) in UEFI systems. The BIOS/UEFI reads the boot loader from this location and executes it. The boot loader's main job is to locate the kernel image on the disk, load it into memory, and pass control to the kernel.

 Once the boot loader transfers control to the kernel, the kernel takes over the boot process. The kernel initializes essential system components such as process management, memory management, device drivers, and file systems. It also detects and configures hardware devices that were not initialized by the BIOS/UEFI.

 After initializing the kernel, the Unix-based system creates the first user-space process, known as the init process. The init process has process ID [(PID) 1](https://en.wikipedia.org/wiki/Init) and serves as the ancestor of all other processes in the system. Its primary role is to initialize the system environment, start system daemons, and maintain system services.

 The init process continues the system initialization process by executing startup scripts and configuration files. These scripts set up network interfaces, mount file systems, configure system settings, and launch essential system services such as syslogd, cron, and SSH.

 Once the system initialization is complete, the system typically prompts the user to log in either through a text-based login prompt or a graphical login manager, depending on the system configuration. After successful authentication, the user gains access to the system and can start using it for various tasks. 😉👌 @linux_boot_process



## Parallelism and Concurrency in Python

If you have muddle through all of this, and still don't hate me for talking too much, i appreciate it, and to your releif, we are finally going to talk about how to use all of this in Python.

This section onwards is what most programmers will found useful, altghough i shall defend myself from all the talkative allegations by saying that the previous sections are very interesting and useful to understand the next ones, also, i did put a warning at the start of the blog 😅.


### The Global Interpreter Lock (GIL)

We absolutely can not in right mind talk about concurrency in Python without talking about the [Global Interpreter Lock (GIL)](https://wiki.python.org/moin/GlobalInterpreterLock). A *'feature'* which has been probably the most controversial and discussed topic in the Python community since its creation... apart from the migration from Python 2 to Python 3, that's an ugly one too. 

Rembember that Python is an interpreted language, and that the Python interpreter is a program that runs in your computer, which runs an intermidiate, platform agnostic, representation of machine code called *bytecode*, which is usually generated on the fly by the interpreter itself when executing a Python script.

```{python}
# | echo: true
# | code-fold: show
import dis  # <1>


def add(a, b):
    return a + b


dis.dis(add)  # <2>

```
1. Import the `dis` module, which is used to disassemble Python into bytecode.
2. Disassemble the bytecode of the `add` function.

The GIL is a mutex (or lock) that protects access to the Python Interpreter, and it is used to make sure that only one Thread can execute Python code at a time. This is because the Python Interpreter is not thread-safe, meaning that it can not handle multiple Threads executing Python code at the same time without risking corruption of the Python objects and memory leaks.

The reason for this incapability lies in the way Python, or at least CPython, handles memory management. Other programming languages use a technique called *Garbage Collection* to manage memory, which is a way of automatically freeing memory that is no longer in use by having a separate process that runs in the background and frees the memory when it is no longer needed, but Python uses a different technique called [*Reference Counting*](https://en.wikipedia.org/wiki/Reference_counting#:~:text=In%20computer%20science%2C%20reference%20counting,that%20are%20no%20longer%20needed.), where every Python object has a counter that keeps track of how many references to it are in the program, and when the counter reaches zero, the memory is freed.

```{python}
# | echo: true
# | code-fold: show
import sys

a = [1, 2, 3]
b = a
c = a

print(sys.getrefcount(a))  # <1>
```
1. Get the reference count of the `a` list.

This is a very efficient way of managing memory, but it is not thread-safe, because if two Threads are trying to modify the reference count of the same object at the same time, the counter may get corrupted, and the memory may not be freed when it should be, leading to memory leaks and other incredibly hard to debug problems.

The GIL solved it quickly and efficiently, at least for single threaded programs, but it has been a thorn in the side of Python developers for a long time, because it makes it very hard to write concurrent programs in Python, and it makes it very hard to take advantage of the multiple cores in modern CPUs.

Moreover, replacing the GIL hasn't been easy, because almost every solution would involve making single threaded programs, which are the absolute vast majority of Python programs, slower, and that is not a trade-off that the Python Software Foundation is willing to make. 

But i am glad to tell you that in recent years, the Python Software Foundation together with private companies like Microsoft, have been working on a way to replace the GIL without sacrificing single threaded performance. The Python Enhancement Proposal [(PEP) 703](https://peps.python.org/pep-0703/?adobe_mc=MCMID%3D80611954523217342821834267419687567177%7CMCORGID%3DA8833BC75245AF9E0A490D4D%2540AdobeOrg%7CTS%3D1716672968) describes the roadmap for a GIL-optional Python in version 3.13 with slowly making it the default by version 3.16.

:::{.callout-note }

Guess what is one of the official motivators listed in PEP 703 for replacing the GIL? [Deploying AI Models!!!](https://peps.python.org/pep-0703/?adobe_mc=MCMID%3D80611954523217342821834267419687567177%7CMCORGID%3DA8833BC75245AF9E0A490D4D%2540AdobeOrg%7CTS%3D1716672968#the-gil-makes-deploying-python-ai-models-difficult) 😁👌
:::


:::{.callout-warning collapse="false"}

# Compile Python to Bytecode
Did you know you can compile Python code to bytecode 'manually'? You can do it with the `compile` function of the `builtins` module, like so:

```{python}
# | echo: true
# | code-fold: show

code = """         # <1>
def add(a, b):     # <1>
    return a + b   # <1>

print(f'Printing result of add(1, 2): {add(1, 2)}')   # <1>
"""  # <1>

bytecode = compile(code, "<string>", "exec")  # <2>

print(bytecode)
exec(bytecode)  # <3>

```

1. Define a simple Python function. Instead of a string this could have been a `.py` file.
2. Compile the code to bytecode. This functions returns a code object.
3. Execute the bytecode with the `exec` function.


:::


### Multithreading in Python

Finally, we wil see how to use threads in Python, including its synchronization primitives, things like ThreadPools, and we will do a little benchmark to show how CPU-bound multithreaded applications are heavily affected by the GIL.

#### The `threading` and `concurrent.futures` Modules

We have already glimpsed at some multithreading code here and there. This is a more thorough look at the `threading` module in Python.

The common way to create a Thread in Python is by using the `Thread` class from the `threading` module by passing a target function to the constructor.
Then you can start the thread by calling the `start` method, and wait for it to finish by calling the `join` method. You can also pass arguments to the target function by using the `args` parameter and keyword arguments by using the `kwargs` parameter.

```{python}
# | echo: true
# | code-fold: show

from threading import Thread

def print_numbers(n):
    for i in range(n):
        print(i, end=" ")

t = Thread(target=print_numbers, args=(5,))
t.start()
t.join()
```

The `Thread` class is a very low-level way of creating Threads in Python, and it is not very efficient, because creating a new Thread is a very expensive operation, and it is not very flexible, because you can not easily manage a large number of Threads, or pass data between them. 

For these reasons, Python has a higher-level module called `concurrent.futures`, which provides a way of creating and managing Threads in a more efficient and flexible way.

The `concurrent.futures` module provides a class for managing a pool of Threads called `ThreadPoolExecutor`, which is a subclass of the `Executor` class, and it provides a way of submitting tasks to the pool of Threads, and getting the results of the tasks when they are done.

```{python}
# | echo: true
# | code-fold: show

from concurrent.futures import ThreadPoolExecutor

def print_numbers(n):
    for i in range(n):
        print(i, end=" ")

with ThreadPoolExecutor() as executor:
    executor.submit(print_numbers, 5)

```

Other way of creating multiple threads with this class is by using the `map` method, which is a way of submitting multiple tasks to the pool of Threads at once, and getting the results of the tasks when they are done.

```{python}
# | echo: true
# | code-fold: show

from concurrent.futures import ThreadPoolExecutor
import time


def task(n, thread_id):
    l = []
    for i in range(n):
        time.sleep(0.1)
        l.append(f"{thread_id=} {i=}")
    return l


with ThreadPoolExecutor() as executor:
    results = executor.map(task, [5, 5, 5], [1, 2, 3])  # <1>
    for result in results:
        print(result)

```
1. The `map` method takes an iterable of arguments and an iterable of keyword arguments, and it returns an iterable of the results of the tasks.

You can also get the current thread by calling the `current_thread` function from the `threading` module, and you can get the number of Threads in the pool by calling the `max_workers` attribute of the `ThreadPoolExecutor` object.

```{python}
# | echo: true
# | code-fold: show

from concurrent.futures import ThreadPoolExecutor
from threading import current_thread

def task(n):
    print(f"{current_thread().name=} {n=}")
    return n

with ThreadPoolExecutor(max_workers=3) as executor:
    results = executor.map(task, [1, 2, 3, 4, 5])
    for result in results:
        print(result)

```
You can also have data that is local to each thread  by using the `local` class from the `threading` module, which is a subclass of the `local` class from the `threading` module, and it provides a way of creating a thread-local storage, which is a way of storing data that is local to each thread.

```{python}
# | echo: true
# | code-fold: show

from threading import local, Thread


def task(n):
    local_data = local()  # <1>
    local_data.value = n
    print(f"{local_data.value=}")


t1 = Thread(target=task, args=(1,))
t2 = Thread(target=task, args=(2,))
t1.start(), t2.start()
t1.join()
t2.join()

```
1. Create a `local` object to store data that is local to each thread.

Keep in mind that in the case of the `ThreadPoolExecutor` class, the threads are reused, so the data that is local to each thread will be shared between the tasks that are executed by the same thread. For example:

```{python}
# | echo: true
# | code-fold: show
from concurrent.futures import ThreadPoolExecutor
from threading import local
import time
local_data = local()

def task_update(idx): # <1>
    local_data.value = idx  # <1>
    print(f"{idx=} - Updated Value - {local_data.value=}") # <1>
    time.sleep(0.1) # <2>

def task_prior(idx): # <3>
    if hasattr(local_data, "value"): # <3>
        print(f"{idx=} - Prior Value - {local_data.value=}") # <3>
        time.sleep(0.1) 


with ThreadPoolExecutor(max_workers=2) as executor:
    executor.map(task_update, [0, 1]) # <4>
    executor.map(task_prior, [3, 4]) # <5>

```
1. Define a function that updates the value of the `local_data` object.
2. Sleep for a short time to simulate a context switch.
3. Define a function that prints the value of the `local_data` object.
4. Submit tasks to update the value of the `local_data` object. 
5. Submit tasks to print the value of the `local_data` object.

See that when calling the second batch of threads with the `task_prior` function that prints the value of the `local_data`, such value already exists and has been updated by the first batch of threads with the `task_update` function.

So be careful when using the `local` class and reusing threads, because the data that is local to each thread will be shared between the tasks that are executed by the same thread.

There is not much else to Thread management, so let's see how those nasty synchronization primitives work in for multithreading in Python.

#### Syncronization Primitives for Multithreading in Python

Python provides a number of synchronization primitives to help you manage access to shared resources in multithreaded programs. These primitives are available in the `threading` module, and they include:

##### Lock

A simple mutual exclusion lock that can be used to protect shared resources from being accessed by multiple threads at the same time.

```{python}
# | echo: true
# | code-fold: show

from threading import Lock, Thread
import time

lock = Lock()


def task(n):
    with lock: # <1>
        for i in range(n):
            print(i, end=" ")
            time.sleep(0.05)


t1 = Thread(target=task, args=(3,))
t2 = Thread(target=task, args=(3,))
t1.start(), t2.start()
t1.join()
t2.join()

```

1. Use a `Lock` object as a context manager to protect access to the shared resource, in this case, the whole for loop.

The prior example uses a `Lock` object as a context manager, which automatically acquires the lock before entering the block and releases it when exiting the block, crucially it also handles exceptions, so the lock is always released, even if an exception is raised.

You can aquire and realease a Lock manually, although it is not recommended, because it is easy to forget to release the lock, and it can lead to deadlocks. But here is the same example as before with manual lock management:

```{python}
# | echo: true
# | code-fold: show

from threading import Lock, Thread
import time

lock = Lock()


def task(n):
    try:
        lock.acquire()  # <1>
        for i in range(n):
            print(i, end=" ")
            time.sleep(0.05)
    except Exception as e:
        print(e)
    finally:
        lock.release()  # <2>


t1 = Thread(target=task, args=(3,))
t2 = Thread(target=task, args=(3,))
t1.start(), t2.start()
t1.join()
t2.join()

```

1. Manual aquiring the lock..
2. When manually managing locks, it is important to release the lock in a `finally` block to ensure that it is always released, even if an exception is raised.

##### RLock

A reentrant lock that can be acquired multiple times by the same thread.

```{python}
# | echo: true
# | code-fold: show

from threading import RLock


class RecursiveCounter:
    def __init__(self):
        self.count = 0
        self.lock = RLock()

    def increment(self, times):
        with self.lock:
            if times > 0:
                self.count += 1
                self.increment(times - 1)  # <1>


# Create a RecursiveCounter instance
counter = RecursiveCounter()

# Increment the counter recursively
counter.increment(5)

# Print the final count
print("Counter final value:", counter.count)

```

1. Recursively call the `increment` method. With a normal `Lock` this would cause a deadlock.


##### Semaphore 
A counter that can be used to control access to a shared resource by a fixed number of threads.

```{python}
# | echo: true
# | code-fold: show

from threading import Semaphore, Thread
import time

# Create a semaphore that allows up to two threads to enter a section of code at once
semaphore = Semaphore(2) # <1>

def thread_function(num): # <2>
    print(f"Thread {num} is waiting for the semaphore") # <2>
    with semaphore: # <2>
        print(f"Thread {num} has acquired the semaphore") # <2>
        time.sleep(0.05)  # Simulate some work # <2>
    print(f"Thread {num} has released the semaphore") # <2>

# Create 5 threads that will each try to acquire the semaphore
threads = [Thread(target=thread_function, args=(i,)) for i in range(5)]

# Start all threads
for t in threads:
    t.start()

# Wait for all threads to finish
for t in threads:
    t.join()

```

1. Create a semaphore that allows up to two threads to enter a section of code at once.
2. Use the semaphore as a context manager to control access to the shared resource.

##### Event

A simple way to communicate between threads using a flag that can be set or cleared.


```{python}
# | echo: true
# | code-fold: show

from threading import Event, Thread
import time

# Create an Event object
event = Event() # <3>

def waiting_thread(): # <1>
    print("Waiting for the event to be set") # <1>
    event.wait()  # <1>
    print("The event has been set, proceeding") # <1>

def setting_thread(): # <2>
    time.sleep(0.05)  # Simulate some work # <2>
    print("Setting the event") # <2>
    event.set() # <2>

# Create the threads
t1 = Thread(target=waiting_thread)
t2 = Thread(target=setting_thread)

# Start the threads
t1.start()
t2.start()

# Wait for both threads to finish
t1.join()
t2.join()

```

1. The `waiting_thread` function waits for the event to be set before proceeding
2. The `setting_thread` function sets the event after a delay.
3. The `Event` object allows these two threads to synchronize their actions.


An Event is a simple synchronization object; the main methods are `set()` and `clear()`. If the Event is set, `wait()` doesn't do anything. If the Event is not set, `wait()` will block until the Event is set.

##### Condition

A more advanced synchronization primitive that allows threads to wait for a condition to be met before proceeding.

```{python}
# | echo: true
# | code-fold: show

from threading import Condition, Thread

# Create a Condition object
condition = Condition()


def consumer_thread():  # <1>
    with condition:  # <1>
        print("Consumer: Waiting for the condition to be met")  # <1>
        condition.wait()  # <1>
        print("Consumer: The condition has been met, proceeding")  # <1>


def producer_thread():  # <2>
    with condition:  # <2>
        print("Producer: Making the condition true")  # <2>
        condition.notify_all()  # <2>


# Create the threads
t1 = Thread(target=consumer_thread)
t2 = Thread(target=producer_thread)

# Start the threads
t1.start()
t2.start()

# Wait for both threads to finish
t1.join()
t2.join()

``` 
1. The `consumer_thread` function waits for the condition to be met before proceeding.
2. The `producer_thread` function sets the condition and notifies the waiting threads.


A `Condition` object allows one or more threads to wait until they are notified by another thread.

##### Barrier

A synchronization primitive that allows a fixed number of threads to wait for each other at a barrier before proceeding.
  
```{python}
# | echo: true
# | code-fold: show

from threading import Barrier, Thread

# Create a Barrier for three threads
barrier = Barrier(3)

def worker_thread(num): # <1>
    print(f"Thread {num} is doing some work") # <1>
    # Simulate work with a sleep # <1>
    time.sleep(num/10) # <1>
    print(f"Thread {num} is waiting at the barrier") # <1>
    barrier.wait() # <1>
    print(f"Thread {num} is proceeding")  # <1>

# Create three worker threads
threads = [Thread(target=worker_thread, args=(i,)) for i in range(3)]

# Start all threads
for t in threads:
    t.start()

# Wait for all threads to finish
for t in threads:
    t.join()

```

1. The `worker_thread` function simulates work and waits at the barrier before proceeding.

##### Queue

A thread-safe queue that can be used to pass messages between threads.

```{python}
# | echo: true
# | code-fold: show

from threading import Thread
from queue import Queue

# Create a Queue object
q = Queue()

def producer_thread(): # <1>
    for i in range(5): # <1>
        q.put(i) # <1>
        print(f"Produced: {i}") # <1>

def consumer_thread(): # <2>
    while True: # <2>
        item = q.get() # <2>
        if item is None: # <2>
            break # <2>
        print(f"Consumed: {item}") # <2>

# Create the threads
t1 = Thread(target=producer_thread)
t2 = Thread(target=consumer_thread)

# Start the threads
t1.start()
t2.start()

# Wait for the producer thread to finish
t1.join()

# Signal the consumer thread to stop
q.put(None) # <3>

# Wait for the consumer thread to finish
t2.join()

```

1. The `producer_thread` function puts items into the queue.
2. The `consumer_thread` function gets items from the queue and processes them.
3. The `None` item is used to signal the consumer thread to stop, if this is not done, the consumer thread will block indefinitely.

So we can have fun with all of these synchronization primitives, but the cases for multithreading in Python are very specific, thank to the GIL. In general, CPU-bound tasks, which are those that require a lot of computation, are not a good fit for multithreading in Python, because the GIL will prevent the threads from running in parallel, and the performance will be worse than using a single thread.

Let's see a simple example of performance degradation when using multithreading for a CPU-bound task in Python.

#### Benchmarking Multithreading in Python in a CPU-Bound Task



### Multiprocessing in Python


### Asynchronous Programming in Python

## Practical Examples

### (CPU-Bound) Speeding up Image Pre-Processing Tasks

### (I/O-Bound) Speeding up Batch Calls to the OpenAI API

## Conclusion

...