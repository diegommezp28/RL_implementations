{
  "hash": "3ac3198ea71ff76de849103614564888",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: On Multithreading and Multiprocessing for Machine Learning in Python\ndescription: 'A deep dive into multithreading and multiprocessing, its relation with the OS, and how to use it in Python for speeding up Machine Learning tasks.'\nauthor: Diego Andrés Gómez Polo\ndate: 2/15/2024\ndraft: true\nformat:\n  html:\n    code-fold: true\n    code-summary: Show source\n    code-copy: true\n    smooth-scroll: true\nbibliography: ref.bib\ncategories:\n  - en\n  - ai\n  - python\n  - multithreading\n  - multiprocessing\ntoc: true\ntoc-location: left-body\ncomments:\n  utterances:\n    repo: diegommezp28/RL_implementations\n    label: blog-comments\n    theme: github-dark-orange\n    issue-term: pathname\nwebsite:\n  twitter-card: true\n---\n\n## Motivation\n\nIf you've worked in software for long enough, you've probably heard in the wild people throwing around terms like *multithreading*, *multiprocessing*, *asynchronous programming*, *concurrency*, *parallelism*, and so on. Often, and mistakenly, these terms are used interchangeably, to refere to stuff thet happens *'at the same time'* in a computer program. You will see in this blog how most of the time this *'parallel behaviour'* is actually a carefully orchestrated illusion by the OS kernel and the CPU that lies at the heart of your computer. You will also learn how these concepts apply to Python and ML tasks in both CPU-bound and I/O-bound scenarios (don't worry, we will define these terms later ;D) and how a proper understanding of these concepts can help you speed up your ML tasks.\n\n::: {.column-margin}\n**Personal Motivation: **\n\nThe contents of this blog are mostly based on the first 2 weeks of the CUDA Cohort by [Cohere for AI](https://cohere.com/research), an OpenScience Community which i am part of. \n\nKudos to the C4AI team for the amazing content and the great community they are building.\n:::\n\n::: {.callout-warning}\n## Warning\n\nSince this blog is in part a note to myself and in part a guide for others, there will be mixed didacticle and highly technical content. I will try to keep a balance and annotate the most technical parts with a warning like this one, so you can skip them if you are not interested in the nitty-gritty details.\n:::\n\n## Parallelism vs Concurrency\n\nSometimes, many programmers, even experienced ones, think that by adding more threads to a program it will magically become faster, and is not rare that when they actually do this, the program becomes slower, and they are left scratching their heads, questioning every piece of knowledge they have recieved about how a computer should work.\n\nThe essence of this confussion is, many times, due to a misunderstanding of these two concepts: *Parallelism* and *Concurrency*. \n\nThere are many great resources on the internet which try to explain the difference between the two, but in my opinion many of them actually get it wrong. A good explanation i've found is by [Rob Pike](https://en.wikipedia.org/wiki/Rob_Pike) in his talk [Concurrency is not Parallelism](https://youtu.be/oV9rvDllKEg?si=djukEXBpXNq5hAyR). In this talk, Rob Pike explains that *concurrency* is about **dealing with** lots of things at once, while *parallelism* is about **doing** lots of things at once. \n\nThis may seem rather abstract, so to clear things out, i am aspiring to have a very precise wording in my explanations: Concurrency is a property of the *structure* of a program, while Parallelism talks about the *execution* of a program. A concurrent program may be run completely sequentially, and actually most of the time this is the case, but it has a structure such that parts of it ***can*** be executed in parallel, potentially making it faster.\n\nThis is best seen with the following diagram:\n\n![Structure of a concurrent programm](images/Conc vs Parall.svg){.column-body-outset #fig-conc-struct width=800px}\n\nIn @fig-conc-struct we can see the abstract representation of a program that has to complete many tasks ($T_0, T_0^1, T_0^2, \\cdots, T_2, T_3$) before exiting. \n\nCrucially, this is **NOT** representing how it runs, but it represents the *dependencies* between the tasks. For example, task $T_2^1$ needs that $T_1^1$ is completed before it runs but doesn't really care about $T_0^3$, whether it has already run, will run or will never run, in the same way,  $T_0^3$ needs for $T_0$ to be completed before it runs but doesn't really care about $T_2^1$. \n\n::: {.column-margin}\n**A Note for the Data Engineers**\n\nThose of you who have worked with Directed Acyclic Graphs (DAGs) in the context of data processing frameworks like [Apache Airflow](https://airflow.apache.org/), [Dagster](https://dagster.io/), or [Dask](https://www.dask.org/), may have already identified the structure of a concurrent program as a DAG. This is not a coincidence, as it is precisely the exact same thing, where in this case the nodes are the instructions of the program and the edges are the dependencies between them.\n\n\nYou will see in the following section that your good old Airflow Scheduler is very similar to the OS Kernel Scheduler, but on a higher level of abstraction.\n:::\n\nThe important observation here is that because of this lack of dependency between $T_0^3$ and $T_2^1$, they are ***suited for parallel execution*** and if executed as such, can potentially make the program faster.\n\nFor a more intuitive view, you can think of the tasks that a program runs like the functions it calls, some functions require a prior call to other functions to run properly (probably the function `read_file` needs that we first call a function `open_file`) and some functions don't really care about the result of others (you can probably run `print(\"Hello World\")` at any point in your program without any problem). \n\nUnderstanding what parts of your program constitute a block of tasks ***suited for parallel execution*** is the first step to make it faster. \n\nI hope is also clear by now that *Parallelism* is just a synonym for *Parallel Execution*. So, indeed, this is the proper word to use when you want to refer to something *\"running at the same time\"* in a computer program.\n\nNow, let's step away from the world of theory and actually get to know our computers...\n\n\n\n::: {.column-margin}\nIf we get really precise, the tasks in @fig-conc-struct are actually CPU-level instructions, but for the sake of simplicity we will think of them as functions. For now...\n:::\n\nIf our CPUs had unlimited parallelism, we could just throw all the tasks *suited for parallel execution* at it and it would run them all at the same time, making our programs run faster. But, as you will see in the next section, this is not the case, and we need to understand how our computers work to make the most out of them. \n\nI am crossing my fingers so that, when you finish reading the next section, you will not be that poor programmer scratching his head, after creating 128 threads in a Core i5 processor.\n\n> **TL;DR:** Concurrency is about the structure of a program, while Parallelism is about its execution\n\n::: {.callout-warning collapse=true .column-body-outset}\n## Concurrency, DAGs and Strict Partial Orders\n\nIf you have a little bit more of a formal background you may have already identified that the concept of *Concurrent Structure* later presented can be precisely defined in terms of basic set theory and binary relations. In concrete, the structure of a concurrent program can be seen as a [Strict Partial Order](https://en.wikipedia.org/wiki/Partially_ordered_set#Strict_partial_orders) where the elements of the set are the tasks $t \\in T$ of the program and the relation $R \\subset T \\times T$ is the *happens-before* relation, where $\\{t, t'\\} \\in R$ **iff** we have that $t$ *happens-before* of $t'$. Moreover, 2 tasks $t$ and $t'$ are *Suited for parallel execution if and only if they are not comparable in the partial order* , that is, $\\{t, t'\\} \\notin R$ and $\\{t', t\\} \\notin R$. \n\nIt is also worth noting that the notation used in @fig-conc-struct ist **not** generalizable to all concurrent programs. If it were, that would imply the existence of a constant time algorithm $\\mathcal{O}(1)$ with linear space $\\mathcal{O}(E)$ which finds if two nodes are connected in a [Directed Acyclic Graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph#:~:text=A%20directed%20acyclic%20graph%20is,a%20path%20with%20zero%20edges) (DAG) *without* executing any path traversal algorithm. This would be the greatest solution ever discovered to calculating the [Transitive Clousure](https://en.wikipedia.org/wiki/Transitive_closure#In_graph_theory) of a Graph.... :p\n:::\n\n\n\n## Parrallelism and Concurrency in your Computer\n\n### CPU Basics\n\n### How does the OS Kernel Interacts with the CPU\n\n#### What is a Process?\n\n#### What is a Thread?\n\n#### What is a Context Switch?\n\n#### What is a Core?\n\n### OS Kernel Scheduler\n\n#### Process Queues\n\n#### Round Robin Scheduling\n\n### (Off-topic) Hyperthreading\n\n### (Off-topic) How Does Your Computer Boot?\n\n## Parallelism and Concurrency in Python\n\n### The Global Interpreter Lock (GIL)\n\n### Multithreading in Python\n\n### Multiprocessing in Python\n\n### Asynchronous Programming in Python\n\n\n## Practical Examples\n\n### CPU-bound Tasks \n\n#### Speeding up image processing tasks\n\n\n### I/O-bound Tasks\n\n#### Speeding up calls to the OpenAI API\n\n## Conclusion\n\n\n\n## Glossary\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}