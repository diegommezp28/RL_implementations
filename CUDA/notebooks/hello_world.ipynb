{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nvcc4jupyter\n",
      "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: nvcc4jupyter\n",
      "Successfully installed nvcc4jupyter-1.2.1\n",
      "Source files will be saved in \"C:\\Users\\diego\\AppData\\Local\\Temp\\tmpp9reof93\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\diego\\anaconda3\\envs\\torch_last\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\diego\\anaconda3\\envs\\torch_last\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\diego\\anaconda3\\envs\\torch_last\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\diego\\anaconda3\\envs\\torch_last\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\diego\\anaconda3\\envs\\torch_last\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\diego\\anaconda3\\envs\\torch_last\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\diego\\anaconda3\\envs\\torch_last\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install nvcc4jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source files will be saved in \"C:\\Users\\diego\\AppData\\Local\\Temp\\tmpulwbcf4s\".\n"
     ]
    }
   ],
   "source": [
    "%load_ext nvcc4jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from block: 0, thread: 0\n",
      "Hello from block: 0, thread: 1\n",
      "Hello from block: 1, thread: 0\n",
      "Hello from block: 1, thread: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void hello(){\n",
    "    printf(\"Hello from block: %u, thread: %u\\n\", blockIdx.x, threadIdx.x);\n",
    "}\n",
    "\n",
    "int main(){\n",
    "    hello<<<2, 2>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n",
      "Device Number: 0\n",
      "  Device name: NVIDIA GeForce RTX 2060\n",
      "  Memory Clock Rate (MHz): 6836\n",
      "  Memory Bus Width (bits): 192\n",
      "  Peak Memory Bandwidth (GB/s): 336.0\n",
      "  Total global memory (Gbytes) 6.0\n",
      "  Shared memory per block (Kbytes) 48.0\n",
      "  minor-major: 5-7\n",
      "  Warp-size: 32\n",
      "  Concurrent kernels: yes\n",
      "  Concurrent computation/communication: yes\n",
      "\n",
      "no error\n",
      "no error\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#\n",
    "#include <stdio.h>\n",
    "#include <string>\n",
    "#include <iostream>\n",
    "\n",
    "int main() {\n",
    "\n",
    "  int nDevices;\n",
    "  cudaGetDeviceCount(&nDevices);\n",
    "  \n",
    "  printf(\"Number of devices: %d\\n\", nDevices);\n",
    "  \n",
    "  for (int i = 0; i < nDevices; i++) {\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, i);\n",
    "    printf(\"Device Number: %d\\n\", i);\n",
    "    printf(\"  Device name: %s\\n\", prop.name);\n",
    "    printf(\"  Memory Clock Rate (MHz): %d\\n\",\n",
    "           prop.memoryClockRate/1024);\n",
    "    printf(\"  Memory Bus Width (bits): %d\\n\",\n",
    "           prop.memoryBusWidth);\n",
    "    printf(\"  Peak Memory Bandwidth (GB/s): %.1f\\n\",\n",
    "           2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
    "    printf(\"  Total global memory (Gbytes) %.1f\\n\",(float)(prop.totalGlobalMem)/1024.0/1024.0/1024.0);\n",
    "    printf(\"  Shared memory per block (Kbytes) %.1f\\n\",(float)(prop.sharedMemPerBlock)/1024.0);\n",
    "    printf(\"  minor-major: %d-%d\\n\", prop.minor, prop.major);\n",
    "    printf(\"  Warp-size: %d\\n\", prop.warpSize);\n",
    "    printf(\"  Concurrent kernels: %s\\n\", prop.concurrentKernels ? \"yes\" : \"no\");\n",
    "    printf(\"  Concurrent computation/communication: %s\\n\\n\",prop.deviceOverlap ? \"yes\" : \"no\");\n",
    "  }\n",
    "\n",
    "  cudaError_t cuda_error = cudaGetLastError();\n",
    "  std::cout << cudaGetErrorString(cuda_error) << \"\\n\" ;\n",
    "  std::cout << cudaGetErrorString(cudaSuccess) ;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "no error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "\n",
    "// Lets try to break things\n",
    "\n",
    "int main(){\n",
    "    char* var = \"Hello\";        // var points to a string literal \"Hello\"\n",
    "    char* nul_char = (var + 5); // nul_char points to the null terminator character '\\0' in the string \"Hello\"\n",
    "    //*nul_char = 'a';            // Attempting to modify the null terminator character ('\\0') to 'a'\n",
    "    cout << var << '\\n';    \n",
    "\n",
    "    cudaError_t cuda_error = cudaGetLastError();\n",
    "    cout << cudaGetErrorString(cuda_error) << \"\\n\" ;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Wolrd!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <iostream>\n",
    "using namespace std;\n",
    "\n",
    "\n",
    "auto main() -> int {\n",
    "    cout << \"Hello Wolrd!\\n\";\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_last",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
