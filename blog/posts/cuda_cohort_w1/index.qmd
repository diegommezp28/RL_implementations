---
title: "On Multithreading and Multiprocessing for Machine Learning in Python"
description: "A deep dive into multithreading and multiprocessing, its relation with the OS, and how to use it in Python for speeding up Machine Learning tasks."
author: "Diego Andrés Gómez Polo"
date: "2/15/2024"
draft: true
format:
  html:
    code-fold: true
    code-summary: "Show source"
    code-copy: true
    smooth-scroll: true
    html-math-method: katex
    # mainfont: Lora
    fontsize: 16pt
    grid:
      body-width: 1000px
    css: 
      - styles.css
bibliography: ref.bib
csl: ieee-with-url.csl
# title-block-banner: images/banner.png
# title-block-banner-color: black
categories: ["en", "ML", "python", "multithreading", "multiprocessing"]
# image: images/thumbnail.png
toc: true
toc-location: left-body
comments: 
  utterances:
    repo: diegommezp28/RL_implementations
    label: blog-comments
    theme: github-dark-orange
    issue-term: pathname
website:
  # open-graph: 
  #   image: images/thumbnail.png
  twitter-card: true
editor:
  render-on-save: true
filters:
  - parse-latex


---

:::{.callout-important}
**DRAFT - WORK IN PROGRESS**
:::

## Motivation

If you've worked in software for long enough, you've probably heard in the wild people throwing around terms like *multithreading*, *multiprocessing*, *asynchronous programming*, *concurrency*, *parallelism*, and so on. Often, and mistakenly, these terms are used interchangeably, to refere to stuff thet happens *'at the same time'* in a computer program. You will see in this blog how most of the time this *'parallel behaviour'* is actually a carefully orchestrated illusion by the OS kernel and the CPU that lies at the heart of your computer. You will also learn how these concepts apply to Python and ML tasks in both CPU-bound and I/O-bound scenarios (don't worry, we will define these terms later ;D) and how a proper understanding of these concepts can help you speed up your ML tasks.

::: column-margin
**Personal Motivation:**

The contents of this blog are mostly based on the first 2 weeks of the CUDA Cohort by [Cohere for AI], an OpenScience Community which i am part of.

Kudos to the C4AI team for the amazing content and the great community they are building.
:::

  [Cohere for AI]: https://cohere.com/research

::: callout-warning
## Warning

Since this blog is in part a note to myself and in part a guide for others, there will be mixed didacticle and highly technical content. I will try to keep a balance and annotate the most technical parts with a warning like this one, so you can skip them if you are not interested in the nitty-gritty details.
:::

## Parallelism vs Concurrency

Sometimes, many programmers, even experienced ones, think that by adding more threads to a program it will magically become faster, and is not rare that when they actually do this, the program becomes slower, and they are left scratching their heads, questioning every piece of knowledge they have recieved about how a computer should work.

Often times, the essence of this confussion is due to a misunderstanding of these two concepts: *Parallelism* and *Concurrency*.

There are many great resources on the internet which try to explain the difference between the two, but in my opinion regularly they actually get it wrong. A good explanation i've found is by [Rob Pike] in his talk [Concurrency is not Parallelism]. In this talk, Rob Pike explains that *concurrency* is about **dealing with** lots of things at once, while *parallelism* is about **doing** lots of things at once.

  [Rob Pike]: https://en.wikipedia.org/wiki/Rob_Pike
  [Concurrency is not Parallelism]: https://youtu.be/oV9rvDllKEg?si=djukEXBpXNq5hAyR

This may seem rather abstract, so to clear things out, i am aspiring to have a very precise wording in my explanations: Concurrency is a property of the *structure* of a program, while Parallelism talks about the *execution* of a program. A concurrent program may be run completely sequentially, and most of the time this is the case, but it has a structure such that parts of it ***can*** be executed in parallel, potentially making it faster.

This is best seen with the following diagram:

![Structure of a concurrent programm]

  [Structure of a concurrent programm]: images/Conc%20vs%20Parall.svg {#fig-conc-struct .column-body-outset width="800px"}

In @fig-conc-struct we can see the abstract representation of a program that has to complete many tasks ($T_0, T_0^1, T_0^2, \cdots, T_2, T_3$) before exiting.

Crucially, this is **NOT** representing how it runs, but it represents the *dependencies* between the tasks. For example, task $T_2^1$ needs that $T_1^1$ is completed before it runs but doesn't really care about $T_0^3$, whether it has already run, will run or will never run, in the same way, $T_0^3$ needs for $T_0$ to be completed before it runs but doesn't really care about $T_2^1$.

::: column-margin
**A Note for the Data Engineers**

Those of you who have worked with Directed Acyclic Graphs (DAGs) in the context of data processing frameworks like [Apache Airflow], [Dagster], or [Dask], may have already identified the structure of a concurrent program as a DAG. This is not a coincidence, as it is precisely the exact same thing, where in this case the nodes are the instructions of the program and the edges are the dependencies between them.

You will see in the following section that your good old Airflow Scheduler is very similar to the OS Kernel Scheduler, but on a higher level of abstraction.
:::

  [Apache Airflow]: https://airflow.apache.org/
  [Dagster]: https://dagster.io/
  [Dask]: https://www.dask.org/

The important observation here is that because of this lack of dependency between $T_0^3$ and $T_2^1$, they are ***suited for parallel execution*** and if executed as such, can potentially make the program faster.

For a more intuitive view, you can think of the tasks that a program runs like the functions it calls, some functions require a prior call to other functions to run properly (probably the function `read_file` needs that we first call a function `open_file`) and some functions don't really care about the result of others (you can probably run `print("Hello World")` at any point in your program without any problem). [^1]

[^1]: If we get really precise, the tasks in @fig-conc-struct are actually CPU-level instructions, but for the sake of simplicity we will think of them as functions. For now...

Understanding what parts of your program constitute a block of tasks ***suited for parallel execution*** is the first step to make it faster.

I hope is also clear by now that *Parallelism* is just a synonym for *Parallel Execution*. So, indeed, this is the proper word to use when you want to refer to something *"running at the same time"* in a computer program.

Now, let's step away from the world of theory and actually get to know our computers...

If our CPUs had unlimited parallelism, we could just throw all the tasks *suited for parallel execution* at it and it would run them all at the same time, making our programs run faster. But, as you will see in the next section, this is not the case, and we need to understand how our computers work to make the most out of them.

I am crossing my fingers so that, when you finish reading the next section, you will not be that poor programmer scratching his head, after creating 128 threads in a Core i5 processor.

> **TL;DR:** Concurrency is about the structure of a program, while Parallelism is about its execution

::: {.callout-warning .column-body-outset collapse="true"}
## Concurrency, DAGs and Strict Partial Orders

If you have a little bit more of a formal background you may have already identified that the concept of *Concurrent Structure* later presented can be precisely defined in terms of basic set theory and binary relations. In concrete, the structure of a concurrent program can be seen as a [Strict Partial Order] where the elements of the set are the tasks $t \in T$ of the program and the relation $R \subset T \times T$ is the *happens-before* relation, where $\{t, t'\} \in R$ **iff** we have that $t$ *happens-before* of $t'$. Moreover, 2 tasks $t$ and $t'$ are *Suited for parallel execution if and only if they are not comparable in the partial order* , that is, $\{t, t'\} \notin R$ and $\{t', t\} \notin R$.

It is also worth noting that the notation used in @fig-conc-struct ist **not** generalizable to all concurrent programs. If it were, that would imply the existence of a constant time algorithm $\mathcal{O}(1)$ with linear space $\mathcal{O}(E)$ which finds if two nodes are connected in a [Directed Acyclic Graph] (DAG) *without* executing any path traversal algorithm. This would be the greatest solution ever discovered to calculating the [Transitive Clousure] of a Graph.... (Indeed this is me justyfing my simplistic notation :p)
:::

  [Strict Partial Order]: https://en.wikipedia.org/wiki/Partially_ordered_set#Strict_partial_orders
  [Directed Acyclic Graph]: https://en.wikipedia.org/wiki/Directed_acyclic_graph#:~:text=A%20directed%20acyclic%20graph%20is,a%20path%20with%20zero%20edges
  [Transitive Clousure]: https://en.wikipedia.org/wiki/Transitive_closure#In_graph_theory

## Parrallelism and Concurrency in your Computer

Identifying which parts of your program are suited for parallel execution is half of the recipe. let's see the other half.

The CPU or Central Processing Unit is the core component of your machine that is doing the heavy lifting when you run a program. It is the one[^2] doing all the math and logical operations necessary for providing you with a fully rendered and interactive screen running the latest versions of Chrome, Safari, Firefox, or, god forbid, Internet Explorer. But how does it actually do this?

[^2]: Let's forget about GPUs for now, we will cover that in other blogs. Also, in my defense, we lived perfectly okay without them for half of the digital era. ;p

Belive me when i tell you, this is close to magic, and for non-technical folks, it is actually magic, i have seen people on the internet saying that humans could have never invented such a complicated device, and that is the proof that aliens exist and have visited us. But, as you will see, it is actually a very well thought out and engineered piece of technology.

> Yeah, probably most of the magic is on the [Photolithography] process, and the aliens are the [dutch], but i am not that kind of engineer, so i will stick to the software side of things.

  [Photolithography]: https://en.wikipedia.org/wiki/Photolithography
  [dutch]: https://en.wikipedia.org/wiki/ASML_Holding

### CPU Basics

Your CPU consists of three main components: the **Control Unit**, the **Arithmetic Logic Unit (ALU)**, and the **Registers**. This design follows the [von Neumann architecture], which is the basis for most modern computers.

  [von Neumann architecture]: https://en.wikipedia.org/wiki/Von_Neumann_architecture

<!--  Diagram-->

![Simple Diagram of CPU with Von Neumann Architecture]

  [Simple Diagram of CPU with Von Neumann Architecture]: images/CPU_simple.svg {#fig-cpu-simple width="70%"}

Like the conductor of an orchestra swinging the baton, the **Control Unit** is the part of the CPU that reads and interprets instructions from memory. It then directs the operation of the other units by providing control signals. The **Arithmetic Logic Unit (ALU)** is the part of the CPU that performs arithmetic and logical operations. The **Registers** are small, fast storage locations within the CPU that hold data temporarily during processing.

Unlike an orchestra, the tempo or speed in a CPU is not set by the conductor or the *Control Unit*, but by a specific hardware component called the [Clock]. The clock is a device that generates a signal at a constant frequency that is used to synchronize the operation of the CPU. The frequency of the clock is measured in Hertz (Hz) and is the number of clock cycles per second

  [Clock]: https://en.wikipedia.org/wiki/Clock_generator

Your CPU also interacts with external storage and I/O devices via the **Buses** which are basically wires connecting things. There are many kinds of buses which allow the CPU to talk to different components like the RAM, the SSD, the Keyboard, the Display, and so on.

> *But, why do we care about the Registers, Clock and so on if we are talking about parallelism and concurrency?* Bare with me, we are getting there.

Well, it turns out that even though this design is capable of supporting a complete system running many apps, it is ***NOT*** running them all at the same time. In fact, in the simplest case, a given CPU core is only capable of running a single instruction of a very limited [set of instructions] at a time.

  [set of instructions]: https://en.wikipedia.org/wiki/Instruction_set_architecture

Everything your computer ever does must be translated, in one way or another, to a series of these simple instructions [^3], which are interpreted by the Control Unit and then executed either by the Control Unit itself when they involve control flow operations or by the ALU when they involve arithmetic or logical operations.

[^3]: That is why we have [compilers] and why a program needs to be compiled for every different target [Instruction Set Architecture], like [x_64], [x_86], [RISC], etc, since those have different instruction sets

  [compilers]: https://en.wikipedia.org/wiki/Compiler
  [Instruction Set Architecture]: https://en.wikipedia.org/wiki/Comparison_of_instruction_set_architectures
  [x_64]: https://en.wikipedia.org/wiki/X86-64
  [x_86]: https://en.wikipedia.org/wiki/X86
  [RISC]: https://en.wikipedia.org/wiki/Reduced_instruction_set_computer

But clearly you have seen your computer running many things at the same time, right? The music in the background doesn't just stop playing if you open a new tab in your browser, right? Well, in some sense, it does, but it is so fast that you don't notice it.

![Interleaving of Program Instructions]

  [Interleaving of Program Instructions]: images/CPU_interleave.png {#fig-interleaving width="85%"}

To give you the illusion that your computer is running many things at the same time, the OS kernel and the CPU have to do some magic...

In the dark old ages, *prior to 2005*, there was no parallelism AT ALL in your personal PC, scary times indeed my friends :c, all the consumer CPUs in the market followed the standard single core architucture we just described. For them to run many applications without one completly freezing waiting for the other to finish, we basically had a single option, to interleave between programs, taking turns executing some of their instructions: a little bit of word here, OS kernel, some more excel there, OS kernel, let's do networking stuff, OS kernel, and so on, really, really, **REALLY FAST**, and just like that, creating a perfectlly orchestrated illusion of parallelism while providing you with a fully interactive experience.

And if you wanted faster CPUs you just needed to increase the frequency of the clock, interleaving ever faster, without worrying about true parallelism and silly things like that.

#### Multi-core CPUs

But it turns out that the universe has some physical limitations, and we hit a wall of how many clocks we could squeeze in a second back in the early 2000s. This is known as the [Breakdown of Dennard's scaling] and it is the reason why we don't have 10 GHz CPUs in our laptops, basically because they would melt.

  [Breakdown of Dennard's scaling]: https://en.wikipedia.org/wiki/Dennard_scaling#Breakdown_of_Dennard_scaling_around_2006

So instead of increasing the frequency of the clock, we started adding more cores to the CPU, striving for **True Parallelism**.

Thanks to this new design, then we didn't have a single core running many things, we then had many cores running many things, and the OS kernel has to orchestrate the execution of all of them, giving them time to run their instructions and taking them out of the CPU at specific interlvals, to let the rest of programs execute some of their instructions. What we saw in @fig-interleaving now looks more like this:

![Interleaving Instructions in a Multi-core CPU]

  [Interleaving Instructions in a Multi-core CPU]: images/Multi_core_cpu.png {#fig-interleaving-multi-core width="85%"}

Stuff stills gets interleaved, but now we have many interleaving streams, allowing for true parallelism.

Crearly the OS kernel has to be a little bit more sophisticated now, implementing what people call [Symmetric Multi Processing], but the basic idea is the same, it has to give execution time to every program that wants to run, and it has to do it in a way that is fair and efficient.

  [Symmetric Multi Processing]: https://linux-kernel-labs.github.io/refs/heads/master/lectures/smp.html

*'Fair and efficient'* is a very broad term, and it is actually a very hard problem to solve, but the OS kernel has some tools to help it with this task, and one of the most important is the ***Scheduler***, which assigns execution time to the ***Threads*** of a ***Process***. We will talk about all of this terms in the following sections, the problems that arise with true parallelism, and how to solve them.

#### Hardware Multi-threading

Before getting into Processes, Threads, Context Switches and all, i shall admit that i've been cutting corners while doing @fig-interleaving and @fig-interleaving-multi-core. It turns out that in modern architectures the ALU is not the single [Execution Unit], but we have a collection of highly specialized circuits that can perform different operations, for example, we can have a circuit for integer operations while another one for floating point operations, called the [Floating-point Unit]. The specific units of execution depends on the [Microarchitecture] of the CPU, but the important thing is that we have more than one.

  [Execution Unit]: https://en.wikipedia.org/wiki/Execution_unit
  [Floating-point Unit]: https://en.wikipedia.org/wiki/Floating-point_unit
  [Microarchitecture]: https://en.wikipedia.org/wiki/Microarchitecture

What this implies in practice is that if we only run one instruction per clock cycle, most likely some part of the processor will be idle, and we would not be making the most out of our hardware.

That is why some great geniuses came up with [Super Scalar Processors] and [Simultaneous Multi-threading] (SMT) which allows for the execution of multiple instructions per clock cycle, by having the CPU dispatch multiple instructions of the [Instruction Queue] per clock cycle, as long as those instructions are meant to be executed by different circuits. [-@Bovet_Cesati_2001]

  [Super Scalar Processors]: https://en.wikipedia.org/wiki/Superscalar_processor
  [Simultaneous Multi-threading]: https://en.wikipedia.org/wiki/Simultaneous_multithreading
  [Instruction Queue]: https://en.wikipedia.org/wiki/Prefetch_input_queue

![CPU Core with Two Execution Threads]

  [CPU Core with Two Execution Threads]: images/CPU_core_2_threads.svg {#fig-hyperthreading width="85%"}

With this architecture, each CPU core can improve its parallelism, without making too many changes to the overall design, and without increasing the power consumption too much.

On most of today's consumer grade CPUs, the CPU exposes 2 execution contexts per core, see @fig-hyperthreading, which are commonly refer to as ***Threads***, and the OS kernel usually treats them as such, giving them execution time as if they were different cores. [-@hyperthreading-intel]

::: callout-note
## Further Reading

We are not Electronic Engineers in here, nor chip designers (or maybe you are, who knows), but this is a fascinating topic, so if you want a deeper dive into this subject, i recommend you resources like [WikiChip] and the [Intel Developer Manual] for a more detailed explanation of the microarchitecture of modern CPUs.
:::

  [WikiChip]: https://en.wikichip.org/wiki/intel/microarchitectures/skylake_(client)#Individual_Core
  [Intel Developer Manual]: https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html

### Programs, Processes, and Threads

From now on, we will, *mostly*, talk about software. From a low level with the OS Kernel, to a very high level with Python, but still, software.

In the last chapter we saw how in most modern CPUs we do have a way of doing actual parallelism, by using multiple CPU cores. Moreover, even inside a single CPU core, we can have multiple *Threads* of execution, which can be used to improve parallelism. But, we also saw that the *OS Kernel* has to orchestrate the execution of all of them.

This magic *OS Kernel* is just another piece of software, but it is a very special one, with all the privileges to do whatever it wants with the hardware, and it is the one that gives execution time to every program that wants to run.

So, for the OS kernel to nicely orchastrate all of this madness, it needs some structure, some way of managing the state of your application and the state of the hardware.

To do so, it creates some very important abstractions for each *Program* you wish to run, and even for itself, and these are the ***Processes*** and ***Threads***.

> Rembember we are talking about software in here, so the term *Thread* is not the same as the physical things we just talked about, but it is very closly related.

Every piece of information about your program, its state, the things it needs to properly run, to be safely interrupted by the Kernel and to resume execution, are nicely packed in these abstractions, which formally are *Data Structures* in the Kernel's memory, but for us, they are just *Processes* and *Threads*.

<!-- There must be some form of predefined structure that gives all the control to the OS kernel and is able to give execution time to every program that wants to run -->

#### Definitions

Let's be precise about these concepts, since they are the key parts a software engineer actually needs to understand to properly program concurrent applications.

A ***Program*** is a set of instructions that are meant to be executed by the CPU. This is the code you write in your favorite programming language, which is then compiled or, in the case of Languages like Python, interpreted by another program called the *Interpreter*. Anyways, the important thing is that a Program is just machine code that the CPU can execute (think, for example, of all of those `.exe` files).

![Process Control Block and Thread Control Block]

  [Process Control Block and Thread Control Block]: images/pcb_tcb.svg {#fig-pcb-tcb width="85%"}

A ***Process*** is an instance of a Program that is being executed by the CPU. This is the actual running of the Program, with all the data structures, memory, and resources that it needs to run properly. The OS Kernel creates a Process for every instance of a Program that you run (if you click on a `.exe` file many times, your OS will create a separate process for each time).

If you open the *Task Manager* in Windows, the *Activity Monitor* in MacOS or run `systemctl` in Linux, you will see a list of all the Processes that are currently running in your computer, and you will see that there are many of them, even if you are not running many applications. This is because the OS Kernel also creates Processes for itself, for the *System Services*, for the *Drivers*, and for every running instance of your favorite browser, text editor, or game.

A ***Thread*** means an *execution thread* within a Process, i.e, a series or list of instructions, from the ones that the Process has in memory, that is meant to be executed.

Crearly each Thread needs the specifics of which instructions from the whole set to execute, and some resources to do it, that is why, for each Thread we also have indepedent [*Program Counter*], [*Registers*], and [*Stack*]. But Threads of the same Process share the same *Memory Space*, including, as said before, the actual code, also, the *File Descriptors* and the *Heap*.

  [*Program Counter*]: https://en.wikipedia.org/wiki/Program_counter
  [*Registers*]: https://en.wikipedia.org/wiki/Processor_register
  [*Stack*]: https://en.wikipedia.org/wiki/Call_stack

*Threads* are also the smallest unit of execution, meaning the OS Kernel will Schedule Threads, not Processes, to run in the CPU. This is because Threads are much lighter than Processes, since they share the same Memory Space, and the OS Kernel can easily switch between them, without having to do a lot of work to save and restore the state of the Process.

::: {.callout-warning collapse="false"}
# Process Control Block (PCB): The Actual Data Structure of a Process

PCB is the generic term used to refer to the Data Structure that an OS employs in order to manage Processes. In the case of the Linux Kernel (the only major one where we can see the code), it is called the [*task_struct*] and it is a very complex structure that holds all the information about a Process. @pcb_5
:::

  [*task_struct*]: https://elixir.bootlin.com/linux/v6.2-rc1/source/include/linux/sched.h#L737

::: {.callout-warning collapse="false"}
# Thread Control Block (TCB): The Actual Data Structure of a Thread

Likewise, TCB is the generic term used to refer to the Data Structure that an OS employs in order to manage Threads. Linux implements it by kind of cheating, because for the Linux Kernel, a Thread is just another *task_struct*, the same as before, but with a pointer to a parent *task_struct* that is the [*parent Process*] for the Thread, also, all the [*children Threads*] of a Process will share some resources but will have some differences specified in the [*thread_info*] struct, and you can create a new children Thread by calling the [`clone()`] system call.

In this sense you may see that in Linux, we have a tree-like structure of tasks, with the root being the first process (PID 0), i.e, the [*init*] process, and every other process being an ancestor of the *init* process, finally, the Threads will then be the leaves of this [tree] of tasks. @pcb_1 @pcb_2 @pcb_3 @pcb_4 @pcb_5

This is in Linux, in other OSs, like Windows, Threads are a separate concept from Processes, and they have their own Data Structure, but the idea is the same, they hold all the information about the Thread, like the Registers, the Stack, the Program Counter, and so on, as specified in @fig-pcb-tcb, but between [*sibling Threads*], they will share the same Memory Space, the same File Descriptors, and the same Heap.
:::

  [*parent Process*]: https://elixir.bootlin.com/linux/v6.2-rc1/source/include/linux/sched.h#L975
  [*children Threads*]: https://elixir.bootlin.com/linux/v6.2-rc1/source/include/linux/sched.h#L983
  [*thread_info*]: https://elixir.bootlin.com/linux/v6.2-rc1/source/arch/alpha/include/asm/thread_info.h#L15
  [`clone()`]: https://man7.org/linux/man-pages/man2/clone.2.html
  [*init*]: https://en.wikipedia.org/wiki/Init#:~:text=Init%20is%20a%20daemon%20process,automatically%20adopts%20all%20orphaned%20processes
  [tree]: https://linuxhandbook.com/show-process-tree/
  [*sibling Threads*]: https://elixir.bootlin.com/linux/v6.2-rc1/source/include/linux/sched.h#L984

::: {.callout-warning collapse="false"}
# Protection Rings and General Hierarchy

The core distinction between the kernel process and all other processes is implemented at the hardware level by using [*Protection Rings*]. These are a set of hardware-enforced protection domains that are used to control the CPU's access to memory, I/O devices, and other resources. The OS kernel runs in the most privileged ring, usually ring 0, while user processes run in a less privileged ring, usually ring 3. This is a security feature that prevents user processes from accessing system resources directly and ensures that the OS kernel has full control over the system.

![Rings of Protection. Fig Source: [Wikipedia]][1]

Modern microarchitectures have many more than 2 protection rings, but mostly for retrocompatibliity reasons, most Operating Systems including Windows, MacOS and Linux, only use 2, the most privileged and the least privileged, where ring 0 is the *Supervisor Mode*, and ring 3 is the *User Mode*. @protection_rings_1 @protection_rings_2

![Only 2 Protection Rings are used in modern Oss. Fig Source: [Wikipedia]](https://upload.wikimedia.org/wikipedia/commons/c/c7/Most_common_protection_rings.svg){#fig-2-protection-rings width="75%"}
:::

  [*Protection Rings*]: https://en.wikipedia.org/wiki/Protection_ring
  [Wikipedia]: https://upload.wikimedia.org/wikipedia/commons/2/2f/Priv_rings.svg
  [1]: https://upload.wikimedia.org/wikipedia/commons/2/2f/Priv_rings.svg {#fig-protection-rings width="75%"}

### OS Kernel Scheduler

We are clear by now that the OS Kernel is the sole administrator of resources in our computers. There are many types of these resources, from memory and files, to the CPU itself. The latter is our subject matter for this chapter.

Access to CPU is quite valuable, most Threads usually want it, so the OS Kernel *Scheduler* has to be very careful in how it gives it.

Processes in modern OSs fall in one of, usually, 5 states, commomly known as the *5 State Model*. These states are: Init, Ready, Running, Waiting and Finished. @process_states_1

![5 State Model. From @process_states_1](images/5_state_model.svg){#fig-5-state-model width="85%"}

:::{.callout-note}
The *5 State Model* is an asbtraction and simplification of the actual states. For example, the Linux Kernel implements de following states: `TASK_RUNNING`, `TASK_INTERRUPTIBLE`, `TASK_UNINTERRUPTIBLE`, `TASK_STOPPED`, `TASK_ZOMBIE`. @process_states_2

:::

The Scheduler will look for the ones that are ready to run, and will give them execution time in the CPU. The way it gives access to CPU is by doing a *Context Switch*, which is the process of saving the state of the currently running Thread, and loading the state of the Thread that is going to run. This is a very expensive operation, since it involves saving and restoring the state of the Registers, the Program Counter, the Stack, and so on, but it is necessary to give execution time to every Thread that wants to run.

#### Time Slices

To have a somewhat simple way of calculating the time a Thread will be allowed to run, the Scheduler has a quanta or minimum, discrete, amount of time that it can give, called the *Time Unit*. So, the assignment of execution time for a given Thread will always be calculated as a fixed integer multiple of this Time Unit.

When a Thread is given Time Units of execution, it is called a *Time Slice*, and the Scheduler will preemptively take the Thread out of the CPU when it has used up all of its *Time Slice*, and will give the CPU to another Thread that is ready to run. This is the essence of a *Preemptive Scheduler*, which is the most common type of Scheduler in modern OSs.

> Note that a time slice **Is NOT** a promise of uninterrupted execution time, but a promise of a minimum amount of total available execution time. The Scheduler can take the Thread out of the CPU at any moment, even before the Time Slice is up, if it decides that another Thread needs to run.

#### Scheduling Algorithm

The way the Scheduler decides which Thread to run next is called the [*Scheduling Algorithm*]. There are many of these, and they are usually classified in two categories: ***Preemptive*** and ***Non-Preemptive***. @priority_levels_1, @scheduler_1, @scheduler_2

  [*Scheduling Algorithm*]: https://www.geeksforgeeks.org/cpu-scheduling-in-operating-systems/

A [Preemptive Scheduler] is pessimistic, and it assumes that any given Thread will run forever if left to its own devices, so they have to be taken forcefully out of the CPU to allow for efficient interleaving of Threads. On the other hand, a Non-Preemptive Scheduler is counting on the goodwill of the Thread to give up its resources from time to time to allow for interleaving, which is not very efficient, but it is much simpler to implement.

  [Preemptive Scheduler]: https://en.wikipedia.org/wiki/Preemption_(computing)

Modern Schedulers are preemptive, and they usually use **Priority Queues** and **Round Robin Scheduling** to decide which Thread to run next.

#### Priority Queues and Priority Levels

The Priority level is how the Scheduler assigns importance to all the Threads that are ready to run. The Thread with the highest priority level will be the one schedule to run next, and the Scheduler will modify the priority level of the Threads as they run, to give execution time to all of them and to ensure that no Thread is left behind.

![Simplified Priority Levels. Source @priority_levels_1](images/priority_levels.svg){#fig-priority-levels width="85%"}

The very specifics of the modifications and updates of priority levels vary by OS kernel implementations. For example, the linux kernel scheduler has been implemented with the [CFS or Completely Fair Scheduler] for the last 15 years, it utilizes a [Red-Black Tree] to store the Threads, and it uses the [Virtual Runtime] of the Threads to calculate the priority level.

  [CFS or Completely Fair Scheduler]: https://en.wikipedia.org/wiki/Completely_Fair_Scheduler
  [Red-Black Tree]: https://en.wikipedia.org/wiki/Red%E2%80%93black_tree
  [Virtual Runtime]: https://www.kernel.org/doc/html/latest/scheduler/sched-design-CFS.html

::: {.callout-warning collapse="false"}
### Earliest eligible virtual deadline first scheduling

[EEVDF] is the scheduler algorithm set to replace CFS. It was described almost 30 years ago, but was only merged in 2023 to the development version 6.6 of the kernel. @scheduler_3 @scheduler_4
:::

  [EEVDF]: https://en.wikipedia.org/wiki/Earliest_eligible_virtual_deadline_first_scheduling

#### Round Robin Scheduling

When the top priority level has been given to more than one Thread, like in @fig-priority-levels, the Scheduler will use a [Round Robin Scheduling] to decide which one to run next. This is a simple algorithm that gives a fixed amount of Time Units to each Thread in a circular fashion, and then starts again. Is desgined to resolve disputes between Threads of the same priority level, and to ensure that all Threads get execution time.

  [Round Robin Scheduling]: https://en.wikipedia.org/wiki/Round-robin_scheduling

![Simplified Round Robing Scheduler. Source @priority_levels_1](images/round_robin.svg){#fig-round-robin width="75%"}

::: {.callout-warning collapse="true"}
### Simulating a Round Robin Scheduler

Round Robin is a very simple, yet effective algorithm, and we can simulate its behaviour even by hand with a toy example:


:::{.LATEX}
**Exercise:**
Consider a system with 3 Threads A, B and C. The threads arrive in the order A, B, C and the arrival time are at 0ms for each thread. The time quantum is 2ms. The threads are scheduled using the Round Robin scheduling algorithm. The context switch time is negligible. The CPU burst times for threads A, B and C are 4ms, 8ms and 6ms respectively. Compute the turnaround time for each thread. 
:::

**Solution:** 
The threads are scheduled as follows:

![Example RR](images/tabla_round_robin.svg){#fig-rr-example width="100%"}

> This example exercise is a modification of a very similar one shown to me in the *Cuda Mini Cohort* at Cohere For AI.

- Thread A runs from 0ms to 2ms
- Thread B runs from 2ms to 4ms
- Thread C runs from 4ms to 6ms
- Thread A runs from 6ms to 8ms
- Thread B runs from 8ms to 10ms
- Thread C runs from 10ms to 12ms
- Thread B runs from 12ms to 14ms
- Thread C runs from 14ms to 16ms
- Thread B runs from 16ms to 18ms
  
The turnaround time for each thread is the time it takes to complete the execution of the thread from the time it arrives. The turnaround time for threads A, B and C are 8ms, 18ms and 16ms respectively. And the average turnaround time is 14ms.

:::

### Problems that Appear in Concurrent Programs

#### Priority Inversion

#### Deadlocks

#### Starvation

#### Race Conditions

### (Off-topic) How Does Your Computer Boot?

## Parallelism and Concurrency in Python

### The Global Interpreter Lock (GIL)

### Multithreading in Python

### Multiprocessing in Python

### Asynchronous Programming in Python

## Practical Examples

### (CPU-Bound) Speeding up Image Pre-Processing Tasks

### (I/O-Bound) Speeding up Batch Calls to the OpenAI API

## Conclusion

...