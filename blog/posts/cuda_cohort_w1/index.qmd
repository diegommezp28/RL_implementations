---
title: "On Multithreading and Multiprocessing for Machine Learning in Python"
description: "A deep dive into multithreading and multiprocessing, its relation with the OS, and how to use it in Python for speeding up Machine Learning tasks."
author: "Diego Andrés Gómez Polo"
date: "2/15/2024"
draft: true
format:
  html:
    code-fold: true
    code-summary: "Show source"
    code-copy: true
    smooth-scroll: true
jupyter: python3
bibliography: ref.bib
csl: ieee-with-url.csl
# title-block-banner: images/banner.png
# title-block-banner-color: black
categories: ["en", "ML", "python", "multithreading", "multiprocessing"]
# image: images/thumbnail.png
toc: true
toc-location: left-body
comments: 
  utterances:
    repo: diegommezp28/RL_implementations
    label: blog-comments
    theme: github-dark-orange
    issue-term: pathname
website:
  # open-graph: 
  #   image: images/thumbnail.png
  twitter-card: true
---

## Motivation

If you've worked in software for long enough, you've probably heard in the wild people throwing around terms like *multithreading*, *multiprocessing*, *asynchronous programming*, *concurrency*, *parallelism*, and so on. Often, and mistakenly, these terms are used interchangeably, to refere to stuff thet happens *'at the same time'* in a computer program. You will see in this blog how most of the time this *'parallel behaviour'* is actually a carefully orchestrated illusion by the OS kernel and the CPU that lies at the heart of your computer. You will also learn how these concepts apply to Python and ML tasks in both CPU-bound and I/O-bound scenarios (don't worry, we will define these terms later ;D) and how a proper understanding of these concepts can help you speed up your ML tasks.

::: {.column-margin}
**Personal Motivation: **

The contents of this blog are mostly based on the first 2 weeks of the CUDA Cohort by [Cohere for AI](https://cohere.com/research), an OpenScience Community which i am part of. 

Kudos to the C4AI team for the amazing content and the great community they are building.
:::

::: {.callout-warning}
## Warning

Since this blog is in part a note to myself and in part a guide for others, there will be mixed didacticle and highly technical content. I will try to keep a balance and annotate the most technical parts with a warning like this one, so you can skip them if you are not interested in the nitty-gritty details.
:::

## Parallelism vs Concurrency

Sometimes, many programmers, even experienced ones, think that by adding more threads to a program it will magically become faster, and is not rare that when they actually do this, the program becomes slower, and they are left scratching their heads, questioning every piece of knowledge they have recieved about how a computer should work.

Often times, the essence of this confussion is due to a misunderstanding of these two concepts: *Parallelism* and *Concurrency*. 

There are many great resources on the internet which try to explain the difference between the two, but in my opinion regularly they actually get it wrong. A good explanation i've found is by [Rob Pike](https://en.wikipedia.org/wiki/Rob_Pike) in his talk [Concurrency is not Parallelism](https://youtu.be/oV9rvDllKEg?si=djukEXBpXNq5hAyR). In this talk, Rob Pike explains that *concurrency* is about **dealing with** lots of things at once, while *parallelism* is about **doing** lots of things at once. 

This may seem rather abstract, so to clear things out, i am aspiring to have a very precise wording in my explanations: Concurrency is a property of the *structure* of a program, while Parallelism talks about the *execution* of a program. A concurrent program may be run completely sequentially, and most of the time this is the case, but it has a structure such that parts of it ***can*** be executed in parallel, potentially making it faster.

This is best seen with the following diagram:

![Structure of a concurrent programm](images/Conc vs Parall.svg){.column-body-outset #fig-conc-struct width=800px}

In @fig-conc-struct we can see the abstract representation of a program that has to complete many tasks ($T_0, T_0^1, T_0^2, \cdots, T_2, T_3$) before exiting. 

Crucially, this is **NOT** representing how it runs, but it represents the *dependencies* between the tasks. For example, task $T_2^1$ needs that $T_1^1$ is completed before it runs but doesn't really care about $T_0^3$, whether it has already run, will run or will never run, in the same way,  $T_0^3$ needs for $T_0$ to be completed before it runs but doesn't really care about $T_2^1$. 

::: {.column-margin}
**A Note for the Data Engineers**

Those of you who have worked with Directed Acyclic Graphs (DAGs) in the context of data processing frameworks like [Apache Airflow](https://airflow.apache.org/), [Dagster](https://dagster.io/), or [Dask](https://www.dask.org/), may have already identified the structure of a concurrent program as a DAG. This is not a coincidence, as it is precisely the exact same thing, where in this case the nodes are the instructions of the program and the edges are the dependencies between them.


You will see in the following section that your good old Airflow Scheduler is very similar to the OS Kernel Scheduler, but on a higher level of abstraction.
:::

The important observation here is that because of this lack of dependency between $T_0^3$ and $T_2^1$, they are ***suited for parallel execution*** and if executed as such, can potentially make the program faster.

For a more intuitive view, you can think of the tasks that a program runs like the functions it calls, some functions require a prior call to other functions to run properly (probably the function `read_file` needs that we first call a function `open_file`) and some functions don't really care about the result of others (you can probably run `print("Hello World")` at any point in your program without any problem). ^[If we get really precise, the tasks in @fig-conc-struct are actually CPU-level instructions, but for the sake of simplicity we will think of them as functions. For now...]

Understanding what parts of your program constitute a block of tasks ***suited for parallel execution*** is the first step to make it faster. 

I hope is also clear by now that *Parallelism* is just a synonym for *Parallel Execution*. So, indeed, this is the proper word to use when you want to refer to something *"running at the same time"* in a computer program.

Now, let's step away from the world of theory and actually get to know our computers...

If our CPUs had unlimited parallelism, we could just throw all the tasks *suited for parallel execution* at it and it would run them all at the same time, making our programs run faster. But, as you will see in the next section, this is not the case, and we need to understand how our computers work to make the most out of them. 

I am crossing my fingers so that, when you finish reading the next section, you will not be that poor programmer scratching his head, after creating 128 threads in a Core i5 processor.

> **TL;DR:** Concurrency is about the structure of a program, while Parallelism is about its execution

::: {.callout-warning collapse=true .column-body-outset}
## Concurrency, DAGs and Strict Partial Orders

If you have a little bit more of a formal background you may have already identified that the concept of *Concurrent Structure* later presented can be precisely defined in terms of basic set theory and binary relations. In concrete, the structure of a concurrent program can be seen as a [Strict Partial Order](https://en.wikipedia.org/wiki/Partially_ordered_set#Strict_partial_orders) where the elements of the set are the tasks $t \in T$ of the program and the relation $R \subset T \times T$ is the *happens-before* relation, where $\{t, t'\} \in R$ **iff** we have that $t$ *happens-before* of $t'$. Moreover, 2 tasks $t$ and $t'$ are *Suited for parallel execution if and only if they are not comparable in the partial order* , that is, $\{t, t'\} \notin R$ and $\{t', t\} \notin R$. 

It is also worth noting that the notation used in @fig-conc-struct ist **not** generalizable to all concurrent programs. If it were, that would imply the existence of a constant time algorithm $\mathcal{O}(1)$ with linear space $\mathcal{O}(E)$ which finds if two nodes are connected in a [Directed Acyclic Graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph#:~:text=A%20directed%20acyclic%20graph%20is,a%20path%20with%20zero%20edges) (DAG) *without* executing any path traversal algorithm. This would be the greatest solution ever discovered to calculating the [Transitive Clousure](https://en.wikipedia.org/wiki/Transitive_closure#In_graph_theory) of a Graph.... (Indeed this is me justyfing my simplistic notation :p)
:::


## Parrallelism and Concurrency in your Computer

Identifying which parts of your program are suited for parallel execution is half of the recipe. let's see the other half.

The CPU or Central Processing Unit is the core component of your machine that is doing the heavy lifting when you run a program. It is the one^[Let's forget about GPUs for now, we will cover that in other blogs. Also, in my defense, we lived perfectly okay without them for half of the digital era. ;p] doing all the math and logical operations necessary for providing you  with a fully rendered and interactive screen running the latest versions of Chrome, Safari, Firefox, or, god forbid, Internet Explorer. But how does it actually do this?

Belive me when i tell you, this is close to magic, and for non-technical folks, it is actually magic, i have seen people on the internet saying that humans could have never invented such a complicated device, and that is the proof that aliens exist and have visited us. But, as you will see, it is actually a very well thought out and engineered piece of technology.

> Yeah, probably most of the magic is on the [Photolithography](https://en.wikipedia.org/wiki/Photolithography) process, and the aliens are the [dutch](https://en.wikipedia.org/wiki/ASML_Holding), but i am not that kind of engineer, so i will stick to the software side of things.



### CPU Basics

Your CPU consists of three main components: the **Control Unit**, the **Arithmetic Logic Unit (ALU)**, and the **Registers**. This design follows the [von Neumann architecture](https://en.wikipedia.org/wiki/Von_Neumann_architecture), which is the basis for most modern computers. 


<!--  Diagram-->
![Simple Diagram of CPU with Von Neumann Architecture](images/CPU_simple.svg){#fig-cpu-simple width=70%}


Like the conductor of an orchestra swinging the baton, the **Control Unit** is the part of the CPU that reads and interprets instructions from memory. It then directs the operation of the other units by providing control signals. The **Arithmetic Logic Unit (ALU)** is the part of the CPU that performs arithmetic and logical operations. The **Registers** are small, fast storage locations within the CPU that hold data temporarily during processing.

Unlike an orchestra, the tempo or speed in a CPU is not set by the conductor or the *Control Unit*, but by a specific hardware component called the [Clock](https://en.wikipedia.org/wiki/Clock_generator). The clock is a device that generates a signal at a constant frequency that is used to synchronize the operation of the CPU. The frequency of the clock is measured in Hertz (Hz) and is the number of clock cycles per second

Your CPU also interacts with external storage and I/O devices via the **Buses** which are basically wires connecting things. There are many kinds of buses which allow the CPU to talk to different components like the RAM, the SSD, the Keyboard, the Display, and so on.


> *But, why do we care about the Registers, Clock and so on if we are talking about parallelism and concurrency? *Bare with me, we are getting there.

Well, it turns out that even though this design is capable of supporting a complete system running many apps, it is ***NOT*** running them all at the same time. In fact, in the simplest case, a given CPU core is only capable of running a single instruction of a very limited [set of instructions](https://en.wikipedia.org/wiki/Instruction_set_architecture) at a time.

Everything your computer ever does must be translated, in one way or another, to a series of these simple instructions ^[That is why we have [compilers](https://en.wikipedia.org/wiki/Compiler) and why a program needs to be compiled for every different target [Instruction Set Architecture](https://en.wikipedia.org/wiki/Comparison_of_instruction_set_architectures), like [x_64](https://en.wikipedia.org/wiki/X86-64), [x_86](https://en.wikipedia.org/wiki/X86), [RISC](https://en.wikipedia.org/wiki/Reduced_instruction_set_computer), etc,  since those have different instruction sets], which are interpreted by the Control Unit and the executed either by the Control Unit itself when they involve control flow operations or by the ALU when they involve arithmetic or logical operations.

But clearly you have seen your computer running many things at the same time, right? The music in the background doesn't just stop playing if you open a new tab in your browser, right? Well, in some sense, it does, but it is so fast that you don't notice it.


![Interleaving of Program Instructions](images/CPU_interleave.png){#fig-interleaving width=85%}

To give you the illusion that your computer is running many things at the same time, the OS kernel and the CPU have to do some magic...

In the dark old ages, *prior to 2005*, there was no parallelism AT ALL in your personal PC, scary times indeed my friends :c, all the consumer CPUs in the market followed the standard single core architucture we just described. For them to run many applications without one completly freezing waiting for the other to finish, we basically had a single option, to interleave between programs, taking turns executing some of their instructions: a little bit of word here, OS kernel, some more excel there, OS kernel, let's do networking stuff, OS kernel, and so on, really, really, **REALLY FAST**, and just like that, creating a perfectlly orchestrated illusion of parallelism while providing you with a fully interactive experience.

And if you wanted faster CPUs you just needed to increase the frequency of the clock, interleaving ever faster, without worrying about true parallelism and silly things like that.

#### Multi-core CPUs

But it turns out that the universe has some physical limitations, and we hit a wall of how many clocks we could squeeze in a second back in the early 2000s. This is known as the [Breakdown of Dennard's scaling](https://en.wikipedia.org/wiki/Dennard_scaling#Breakdown_of_Dennard_scaling_around_2006) and it is the reason why we don't have 10 GHz CPUs in our laptops, basically because they would melt. 

So instead of increasing the frequency of the clock, we started adding more cores to the CPU, striving for **True Parallelism**.

Thanks to this new design, then we didn't have a single core running many things, we then had many cores running many things, and the OS kernel has to orchestrate the execution of all of them, giving them time to run their instructions and taking them out of the CPU at specific interlvals, to let the rest of programs execute some of their instructions. What we saw in @fig-interleaving now looks more like this:

![Interleaving Instructions in a Multi-core CPU](images/Multi_core_cpu.png){#fig-interleaving-multi-core width=85%}

Stuff stills gets interleaved, but now we have many interleaving streams, allowing for true parallelism. 

Crearly the OS kernel has to be a little bit more sophisticated now, implementing what people call [Symmetric Multi Processing](https://linux-kernel-labs.github.io/refs/heads/master/lectures/smp.html), but the basic idea is the same, it has to give execution time to every program that wants to run, and it has to do it in a way that is fair and efficient.

*'Fair and efficient'* is a very broad term, and it is actually a very hard problem to solve, but the OS kernel has some tools to help it with this task, and one of the most important is the ***Scheduler***, which assigns execution time to the ***Threads*** of a ***Process***. We will talk about all of this terms in the following sections, the problems that arise with true parallelism, and how to solve them. 

#### Hardware Multi-threading


Before getting into Processes, Threads, Context Switches and all, i shall admit that i've been cutting corners while doing @fig-interleaving and @fig-interleaving-multi-core. It turns out that in modern architectures the ALU is not the single [Execution Unit](https://en.wikipedia.org/wiki/Execution_unit), but we have a collection of highly specialized circuits that can perform different operations, for example, we can have a circuit for integer operations while another one for floating point operations, called the [Floating-point Unit](https://en.wikipedia.org/wiki/Floating-point_unit). The specific units of execution depends on the [Microarchitecture](https://en.wikipedia.org/wiki/Microarchitecture) of the CPU, but the important thing is that we have more than one.

 What this implies in practice is that if we only run one instruction per clock cycle, most likely some part of the processor will be idle, and we would not be making the most out of our hardware. 

That is why some great geniuses came up with [Super Scalar Processors](https://en.wikipedia.org/wiki/Superscalar_processor) and [Simultaneous Multi-threading](https://en.wikipedia.org/wiki/Simultaneous_multithreading) (SMT) which allows for the execution of multiple instructions per clock cycle, by having the CPU dispatch multiple instructions of the [Instruction Queue](https://en.wikipedia.org/wiki/Prefetch_input_queue) per clock cycle, as long as those instructions are meant to be executed by different circuits. [-@Bovet_Cesati_2001]

![CPU Core with Two Execution Threads](images/CPU_core_2_threads.svg){#fig-hyperthreading width=85%}

With this architecture, each CPU core can improve its parallelism, without making too many changes to the overall design, and without increasing the power consumption too much. 

On most of today's consumer grade CPUs, the CPU exposes 2 execution contexts per core, see @fig-hyperthreading, which are commonly refer to as ***Threads***, and the OS kernel usually treats them as such, giving them execution time as if they were different cores. [-@hyperthreading-intel]


::: {.callout-note}
## Further Reading
We are not Electronic Engineers in here, nor chip designers (or maybe you are, who knows), but this is a fascinating topic, so if you want a deeper dive into this subject, i recommend you resources like [WikiChip](https://en.wikichip.org/wiki/intel/microarchitectures/skylake_(client)#Individual_Core) and the [Intel Developer Manual](https://software.intel.com/content/www/us/en/develop/articles/intel-sdm.html) for a more detailed explanation of the microarchitecture of modern CPUs.
:::

### How does the OS Kernel Interacts with the CPU

From now on, we will, *mostly*, talk about software. From a low level with the OS Kernel, to a very high level with Python, but still, software.

In the last chapter we saw how in most modern CPUs we do have a way of doing actual parallelism, by using multiple CPU cores. Moreover, even inside a single CPU core, we can have multiple *Threads* of execution, which can be used to improve parallelism. But, we also saw that the *OS Kernel* has to orchestrate the execution of all of them. 

This magic *OS Kernel* is just another piece of software, but it is a very special one, with all the privileges to do whatever it wants with the hardware, and it is the one that gives execution time to every program that wants to run.

So, for the OS kernel to nicely orchastrate all of this madness, it needs some structure, some way of managing the state of your application and the state of the hardware. 

To do so, it creates some very important abstractions for each *Program* you wish to run, and even for itself, and these are the ***Processes*** and ***Threads***.

> Rembember we are talking about software in here, so the term *Thread* is not the same as the physical things we just talked about, but it is very closly related.

Every piece of information about your program, its state, the things it needs to properly run, to be safely interrupted by the Kernel and to resume execution, are nicely packed in these abstractions, which formally are *Data Structures* in the Kernel's memory, but for us, they are just *Processes* and *Threads*.


<!-- There must be some form of predefined structure that gives all the control to the OS kernel and is able to give execution time to every program that wants to run -->

#### Programs, Processes, and Threads

Let's be precise about these concepts, since they are the key parts a Software Engineer actually needs to understand to properly program concurrent applications.

A ***Program*** is a set of instructions that are meant to be executed by the CPU. This is the code you write in your favorite programming language, which is then compiled or, in the case of Languages like Python, interpreted by another program called the *Interpreter*. Anyways, the important thing is that a Program is just machine code that the CPU can execute (think, for example, of all of those `.exe` files).

A ***Process*** is an instance of a Program that is being executed by the CPU. This is the actual running of the Program, with all the data structures, memory, and resources that it needs to run properly. The OS Kernel creates a Process for every Program that you run.

If you open the *Task Manager* in Windows, the *Activity Monitor* in MacOS or run `systemctl ` in Linux, you will see a list of all the Processes that are currently running in your computer, and you will see that there are many of them, even if you are not running many applications. This is because the OS Kernel also creates Processes for itself, for the *System Services*, for the *Drivers*, and for every running instance of your favorite browser, text editor, or game.

A ***Thread*** is a unit of execution within a Process. 

A Process can have many Threads, and every Thread has its own *Program Counter*, *Registers*, and *Stack*. But Threads of the same Process share the same *Memory Space*, including the actual code, *File Descriptors*, and *Resources*.

*Threads* are also the smallest unit of execution, meaning the OS Kernel will Schedule Threads, not Processes, to run in the CPU. This is because Threads are much lighter than Processes, since they share the same Memory Space, and the OS Kernel can easily switch between them, without having to do a lot of work to save and restore the state of the Process.

#### Process Control Block (PCB): The Actual Data Structure of a Process

#### What is a Context Switch?


### OS Kernel Scheduler

#### Process Queues

#### Round Robin Scheduling

### Problems that Appear in Concurrent Programs

#### Deadlocks

#### Starvation


#### Race Conditions


#### Priority Inversion

### (Off-topic) How Does Your Computer Boot?

## Parallelism and Concurrency in Python

### The Global Interpreter Lock (GIL)

### Multithreading in Python

### Multiprocessing in Python

### Asynchronous Programming in Python


## Practical Examples

### (CPU-Bound) Speeding up Image Pre-Processing Tasks


### (I/O-Bound) Speeding up Batch Calls to the OpenAI API

## Conclusion



## Glossary